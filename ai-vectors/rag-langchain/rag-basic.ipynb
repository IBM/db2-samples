{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Retrieval-Augmented Generation Pipeline Using Db2's Vector Search and LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Notebook Summary: Implementing Retrieval-Augmented Generation (RAG) with IBM Db2 and LangChain**\n",
    "\n",
    "This notebook demonstrates how to build a **Retrieval-Augmented Generation (RAG) pipeline** using **IBM Db2â€™s vector search capabilities** and **LangChain**. The pipeline enhances large language model (LLM) responses by retrieving relevant information from a knowledge base before generating an answer.\n",
    "\n",
    "#### **Key Components & Workflow:**\n",
    "\n",
    "1. **Loading & Processing Documents:**\n",
    "   - `document_loader(url)`: Fetches and cleans text from a given webpage.\n",
    "   - `text_splitter(data)`: Splits the text into **manageable chunks** while maintaining context.\n",
    "\n",
    "2. **Storing & Retrieving Vectors:**\n",
    "   - `watsonx_embedding()`: Converts text chunks into **vector embeddings** using **IBM Watsonx AI**.\n",
    "   - `Db2VectorStore`: Stores the **vectorized document chunks** in **IBM Db2**.\n",
    "   - `retriever(file)`: Retrieves **semantically similar** document chunks based on user queries.\n",
    "\n",
    "3. **Building the RAG Pipeline:**\n",
    "   - `get_llm()`: Loads the **Meta Llama 3 70B** LLM from **IBM Watsonx AI**.\n",
    "   - `qa_chain(file)`: Creates a **retrieval-based QA system** using **retrieved document chunks** and **LLM** to generate context-aware answers.\n",
    "\n",
    "4. **Running the QA System:**\n",
    "   - A sample **URL** (`qa_chain(url)`) is used to load a blog post into the pipeline.\n",
    "   - The system can now **answer questions** based on the content retrieved from IBM Db2.\n",
    "\n",
    "#### **Purpose & Benefits:**\n",
    "- **Prevents hallucinations** by restricting LLM responses to **retrieved knowledge**.\n",
    "- **Improves accuracy** by dynamically **fetching relevant information** before answering.\n",
    "- **Leverages IBM Db2â€™s vector capabilities** to store and search high-dimensional embeddings.\n",
    "- **Creates a reusable RAG pipeline** for answering domain-specific questions.\n",
    "\n",
    "### **Environment Setup**\n",
    "Before running this notebook, make sure you have set up your environment correctly. Follow the **setup instructions** provided in the [README.md](README.md) in the same directory.\n",
    "\n",
    "This notebook serves as a **step-by-step guide** for implementing RAG with **IBM Db2, LangChain, and Watsonx AI**, enabling **more reliable, context-aware AI applications**. ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams, EmbedTextParamsMetaNames\n",
    "from langchain_ibm import WatsonxLLM, WatsonxEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "from langchain.prompts import PromptTemplate\n",
    "from db2_utils import get_db2_connection, close_db2_connection, initialize_db, cleanup_db, Db2VectorStore\n",
    "from IPython.display import display, Markdown\n",
    "from langchain.text_splitter import TokenTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Connecting to Db2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Db2 successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ibm_db.IBM_DBConnection at 0x7f555f814170>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_db2_connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Creating a Db2 table for storing vectors`\n",
    "```sql\n",
    "CREATE TABLE embeddings (\n",
    "    id INT NOT NULL GENERATED ALWAYS AS IDENTITY (START WITH 1, INCREMENT BY 1),\n",
    "    content CLOB,\n",
    "    source VARCHAR(255),\n",
    "    title VARCHAR(255),\n",
    "    embedding VECTOR(1024, FLOAT32),\n",
    "    PRIMARY KEY (id)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'embeddings' dropped successfully (if it existed).\n",
      "Table 'embeddings' created successfully.\n"
     ]
    }
   ],
   "source": [
    "initialize_db()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Load wx.ai API credentials` from `.env` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(os.getcwd()+\"/.env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Supress warnings from code`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Define `RAG` pipeline functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Set up LLM services`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `watsonx_embedding` function **creates and returns an embedding service** using IBM Watsonx for generating vector representations of text.\n",
    "\n",
    "**What it does:**\n",
    "1. **Defines embedding parameters (`embed_params`)**:  \n",
    "   - `TRUNCATE_INPUT_TOKENS: 3` â†’ Limits token truncation.  \n",
    "   - `RETURN_OPTIONS: {\"input_text\": True}` â†’ Ensures the input text is included in the response.  \n",
    "\n",
    "2. **Initializes a `WatsonxEmbeddings` instance**:  \n",
    "   - Uses the `\"intfloat/multilingual-e5-large\"` model for text embeddings.  \n",
    "   - Connects to **IBM Watsonx AI** at `\"https://us-south.ml.cloud.ibm.com\"`.  \n",
    "   - Retrieves API credentials (`apikey` and `project_id`) from **environment variables** (`WATSONX_APIKEY`, `WATSONX_PROJECT`).  \n",
    "\n",
    "3. **Returns the configured Watsonx embedding model**, which can be used to convert text into vector representations for similarity search.\n",
    "\n",
    "### **Purpose:**\n",
    "- Enables **text embeddings** for vector-based retrieval in **RAG pipelines**.  \n",
    "- Supports **multilingual processing** with `multilingual-e5-large`.  \n",
    "- Facilitates **semantic search** in AI applications by converting text into numerical vectors for similarity comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding service for generating vectors\n",
    "def watsonx_embedding():\n",
    "    embed_params = {\n",
    "        EmbedTextParamsMetaNames.TRUNCATE_INPUT_TOKENS: 3,\n",
    "        EmbedTextParamsMetaNames.RETURN_OPTIONS: {\"input_text\": True},\n",
    "    }\n",
    "    watsonx_embedding = WatsonxEmbeddings(\n",
    "        model_id=\"intfloat/multilingual-e5-large\",\n",
    "        url=\"https://us-south.ml.cloud.ibm.com\",\n",
    "        apikey=os.getenv(\"WATSONX_APIKEY\", \"\"),\n",
    "        project_id=os.getenv(\"WATSONX_PROJECT\", \"\"),\n",
    "        params=embed_params,\n",
    "    )\n",
    "    return watsonx_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_llm` function **initializes and returns a large language model (LLM) service** using IBM Watsonx for generating AI-driven responses.\n",
    "\n",
    "**What it does:**\n",
    "1. **Defines the LLM model ID**:  \n",
    "   - Uses **Meta Llama 3 (70B parameters)**: `'meta-llama/llama-3-1-70b-instruct'`, a powerful instruction-tuned language model.\n",
    "\n",
    "2. **Sets generation parameters** (`parameters`):  \n",
    "   - `MAX_NEW_TOKENS: 512` â†’ Limits response length to **512 tokens**.  \n",
    "   - `TEMPERATURE: 0.5` â†’ Balances between **creativity and determinism** in responses (higher values make responses more random).  \n",
    "\n",
    "3. **Creates a Watsonx LLM instance (`WatsonxLLM`)**:  \n",
    "   - Connects to **IBM Watsonx AI** at `\"https://us-south.ml.cloud.ibm.com\"`.  \n",
    "   - Retrieves API credentials (`apikey` and `project_id`) from **environment variables** (`WATSONX_APIKEY`, `WATSONX_PROJECT`).  \n",
    "   - Passes the **model ID** and **generation parameters** for configuration.  \n",
    "\n",
    "4. **Returns the Watsonx LLM instance**, which can be used to generate text-based responses.\n",
    "\n",
    "### **Purpose:**\n",
    "- Enables **natural language generation (NLG)** in AI applications.  \n",
    "- Supports **instruction-following tasks** using **Meta Llama 3**.  \n",
    "- Works in **retrieval-augmented generation (RAG) pipelines** to provide context-aware responses based on retrieved information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm for generating responses\n",
    "def get_llm():\n",
    "    # model_id = 'meta-llama/llama-3-1-70b-instruct'\n",
    "    model_id = 'mistralai/mistral-large'\n",
    "    parameters = {\n",
    "        GenParams.MAX_NEW_TOKENS: 512,\n",
    "        GenParams.TEMPERATURE: 0.5,\n",
    "    }\n",
    "   \n",
    "    watsonx_llm = WatsonxLLM(\n",
    "        model_id=model_id,\n",
    "        url=\"https://us-south.ml.cloud.ibm.com\",\n",
    "        apikey=os.getenv(\"WATSONX_APIKEY\", \"\"),\n",
    "        project_id=os.getenv(\"WATSONX_PROJECT\", \"\"),\n",
    "        params=parameters,\n",
    "    )\n",
    "    return watsonx_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block **defines a prompt template** for a retrieval-augmented generation (RAG) system, ensuring that the AI assistant answers questions **only based on provided context** and avoids hallucination.\n",
    "\n",
    "`What it does`:\n",
    "1. **Defines a structured prompt** (`prompt_template`):  \n",
    "   - Instructs the AI to **only use** the given context to answer.  \n",
    "   - If the answer **is not found in the context**, it must explicitly state:  \n",
    "     *\"The information is not available in the provided context.\"*  \n",
    "2. **Creates a `PromptTemplate` object** (`PROMPT`):  \n",
    "   - Uses `{context}` and `{question}` as placeholders for dynamic input.  \n",
    "   - Ensures the large language model (LLM) receives a **properly formatted** prompt.  \n",
    "\n",
    "### Purpose:\n",
    "- Helps prevent **hallucinations** by restricting responses to known data.  \n",
    "- Enhances the **accuracy and trustworthiness** of LLM-generated answers in **RAG pipelines**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You are a knowledgeable assistant. Answer the question based solely on the provided context.\n",
    "- If the context contains the answer, respond directly to the reader using 'you' to make it personal.\n",
    "- If the answer includes code, provide an explanation of the code following the code block.\n",
    "- If the information is not available in the context, respond with 'The information is not available in the provided context.'\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `document_loader` function **fetches and cleans text content from a webpage** using `WebBaseLoader`.  \n",
    "\n",
    "`What it does`:  \n",
    "1. **Loads webpage content**: It initializes a `WebBaseLoader` with the given URL and loads the page content as `Document` objects.  \n",
    "2. **Cleans the text**: It removes empty lines from the `page_content` of each document to ensure cleaner text.  \n",
    "3. **Returns the extracted content**: The function outputs a list of `Document` objects containing the cleaned webpage content.  \n",
    "\n",
    "This function is useful for **scraping and preprocessing web-based textual data** before further processing (e.g., chunking, embedding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_loader(urls):\n",
    "    \"\"\"\n",
    "    Loads content from a list of webpages using WebBaseLoader.\n",
    "\n",
    "    Args:\n",
    "        urls (str): The URLs of the webpages to load.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of Document objects extracted from the webpage.\n",
    "    \"\"\"\n",
    "    docs = [WebBaseLoader(url).load() for url in urls]\n",
    "    docs_list = [item for sublist in docs for item in sublist]\n",
    "    \n",
    "    return docs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `text_splitter` function **splits a document into smaller chunks** for better processing in retrieval-augmented generation (RAG) pipelines.  \n",
    "\n",
    "`What it does`:  \n",
    "1. **Initializes a text splitter** using `RecursiveCharacterTextSplitter`, which:  \n",
    "   - Splits text into chunks of **1024 characters**.  \n",
    "   - Maintains a **256-character overlap** between chunks to preserve context.  \n",
    "   - Uses `len` as the length function to determine chunk size.  \n",
    "2. **Splits the input data** into chunks using `split_documents(data)`.  \n",
    "3. **Returns the list of document chunks**.  \n",
    "\n",
    "This function ensures that long documents are **divided into manageable segments** while retaining contextual continuity between adjacent chunks, improving **vector search accuracy** and **LLM-generated responses**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_splitter(docs):\n",
    "   # Split\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=2048, chunk_overlap=256\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(docs)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `retriever` function **builds a retrieval system** by processing a document and storing its vectorized chunks for efficient semantic search.  \n",
    "\n",
    " `What it does`:  \n",
    "1. **Loads the document** using `document_loader(file)`.  \n",
    "2. **Splits the document** into smaller chunks using `text_splitter(splits)`.  \n",
    "3. **Creates a vector database** (`Db2VectorStore`) with an embedding function (`watsonx_embedding()`) and retrieves the top **5** most relevant results (`k=5`).  \n",
    "4. **Adds the document chunks** to the vector store for indexing.  \n",
    "5. **Returns a retriever instance** that enables similarity-based search over the stored vectorized chunks.  \n",
    "\n",
    "This function allows **efficient retrieval of relevant document segments** based on a query, improving context-aware AI responses in **retrieval-augmented generation (RAG) pipelines**.\n",
    "\n",
    "`INSERT VECTOR SQL`:\n",
    "```sql\n",
    "INSERT INTO embeddings(content, source, title, embedding)\n",
    "VALUES (?, ?, ?, VECTOR('[{embedding_vector_str}]', 1024, FLOAT32));\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retriever(urls):\n",
    "    docs = document_loader(urls)\n",
    "    chunks = text_splitter(docs)\n",
    "    vectordb = Db2VectorStore(embedding_function=watsonx_embedding(), k=5)\n",
    "    vectordb.add_documents(chunks)\n",
    "    retriever_instance = vectordb.as_retriever()\n",
    "    return retriever_instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `qa_chain` function creates a **question-answering (QA) pipeline** using a retrieval-augmented generation (RAG) approach. It:  \n",
    "\n",
    "1. **Initializes an LLM** using `get_llm()`.  \n",
    "2. **Retrieves relevant document chunks** using the `retriever(file)` function.  \n",
    "3. **Builds a RetrievalQA chain** with `RetrievalQA.from_chain_type()`, specifying the LLM, retrieval method, and prompt.  \n",
    "4. **Returns the QA chain**, which can answer queries based on retrieved documents while also returning source documents.  \n",
    "\n",
    "This function enables **context-aware Q&A** by retrieving relevant information before generating responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_chain(urls):\n",
    "    llm = get_llm()\n",
    "    retriever_obj = retriever(urls)\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever_obj,\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs={\"prompt\": PROMPT}\n",
    "    )\n",
    "    return qa_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Create a QA chain and ask questions using the chain\n",
    "\n",
    "The given code **runs a question-answering (QA) pipeline** using a specified URL as the knowledge source.\n",
    "\n",
    "**What it does:**\n",
    "1. **Defines the URL** .\n",
    "2. **Calls `qa_chain(url)`**:\n",
    "   - Loads the webpage content using `document_loader(url)`.\n",
    "   - Splits the text into **manageable chunks** via `text_splitter`.\n",
    "   - Stores vector embeddings in **Db2VectorStore** using `watsonx_embedding()`.\n",
    "   - Creates a **retrieval-based QA pipeline** (`RetrievalQA`) using `get_llm()` as the language model.\n",
    "3. **Outputs a QA system (`qa_chain`)** that can:\n",
    "   - Retrieve **relevant chunks** from the blog post using **vector search**.\n",
    "   - Generate **context-aware responses** using an **LLM**.\n",
    "\n",
    "**Purpose:**\n",
    "- Enables **automated question-answering** over the blogâ€™s content.\n",
    "- Ensures the **LLM only answers based on retrieved context**.\n",
    "- Supports **retrieval-augmented generation (RAG)** for **accurate, domain-specific AI responses**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'embeddings' dropped successfully (if it existed).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'embeddings' created successfully.\n"
     ]
    }
   ],
   "source": [
    "initialize_db()\n",
    "urls = [\n",
    "    \"https://community.ibm.com/community/user/datamanagement/blogs/shaikh-quader/2024/05/07/building-an-in-db-linear-regression-model-with-ibm\",\n",
    "    \"https://community.ibm.com/community/user/datamanagement/blogs/shaikh-quader/2024/05/27/db2ai-pyudf\"\n",
    "]\n",
    "\n",
    "qa_chain = qa_chain(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Ask LLM\n",
    "\n",
    "Behind the scene SQL for vector search:\n",
    "```sql\n",
    "SELECT \n",
    "    content AS CONTENT, \n",
    "    source AS SOURCE, \n",
    "    title AS TITLE, \n",
    "    VECTOR_DISTANCE(\n",
    "        VECTOR('{query_embedding_str}', 1024, FLOAT32), \n",
    "        embedding, \n",
    "        EUCLIDEAN\n",
    "    ) AS DISTANCE \n",
    "FROM embeddings \n",
    "ORDER BY SIMILARITY ASC \n",
    "FETCH FIRST {top_k} ROWS ONLY;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Question 1: How to use the stored procedure for training a linear regression model in Db2?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "Python UDF stands for Python User-Defined Function. It is a custom function written in Python that can be registered and used within a database system, such as IBM Db2. This allows you to leverage the power and flexibility of Python for tasks that may be complex or not easily achievable with standard SQL functions.\n",
       "\n",
       "In the provided context, a Python UDF is defined to perform batch inferencing using a trained machine learning model. This UDF collects input rows into a batch, processes them using the model, and returns the predictions. The UDF is then registered in the Db2 database, enabling you to use it in SQL queries for generating predictions.\n",
       "\n",
       "Here is the relevant part of the context that defines the Python UDF:\n",
       "\n",
       "```python\n",
       "import nzae\n",
       "import pandas as pd\n",
       "from joblib import load\n",
       "\n",
       "ml_model_path = '/home/shaikhq/pipe_lr/myudf_lr.joblib'\n",
       "ml_model_features = ['YEAR', 'QUARTER', 'MONTH', 'DAYOFMONTH', 'DAYOFWEEK', 'UNIQUECARRIER', 'ORIGIN', 'DEST', 'CRSDEPTIME', 'DEPDELAY', 'DEPDEL15', 'TAXIOUT', 'WHEELSOFF', 'CRSARRTIME', 'CRSELAPSEDTIME', 'AIRTIME', 'DISTANCEGROUP']\n",
       "\n",
       "class full_pipeline(nzae.Ae):\n",
       "    def _runUdtf(self):\n",
       "        # Load the trained pipeline\n",
       "        trained_pipeline = load(ml_model_path)\n",
       "\n",
       "        # Collect rows into a single batch\n",
       "        rownum = 0\n",
       "        row_list = []\n",
       "        for row in self:\n",
       "            if rownum == 0:\n",
       "                # Grab batch size from first element value (select count(*))\n",
       "                batchsize = row[0]\n",
       "\n",
       "            # Collect everything but the first element (which is select count(*))\n",
       "            row_list.append(row[1:])\n",
       "            rownum += 1\n",
       "\n",
       "            if rownum == batchsize:\n",
       "                # Collect data into a Pandas dataframe for scoring\n",
       "                data = pd.DataFrame(row_list, columns=ml_model_features)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieved Contexts:\n",
      "\n",
      "Document 1:\n",
      "Content: # define the strategy to fill in missing values in the categorical columns\n",
      "cat_pipeline = make_pipeline(SimpleImputer(strategy='most_frequent'),\n",
      "                            OneHotEncoder(handle_unknown='ignore'))\n",
      "\n",
      "# combine the previous 2 pipelines into a data preproessing pipeline. \n",
      "\n",
      "preprocessing = make_column_transformer(\n",
      "    (num_pipeline, make_column_selector(dtype_include=np.number)),\n",
      "    (cat_pipeline, make_column_selector(dtype_include='object'))\n",
      ")\n",
      "\n",
      "# create a final pipeline by chaining ...\n",
      "Metadata: 0.6180782797877933\n",
      "\n",
      "Document 2:\n",
      "Content: SELECT count(*) FROM GOSALES.GOSALES_TRAIN\n",
      "\n",
      "\n",
      "\n",
      "SELECT count(*) FROM GOSALES.GOSALES_TEST\n",
      "\n",
      "\n",
      "\n",
      "The above counts confirm that the train and test tables have 80% and 20% of records, respectively, from the original table with 60252 records.\n",
      "Data Exploration\n",
      "Now, I will look into some sample records from the training dataset, GOSALES_TRAIN.\n",
      "SELECT * FROM GOSALES.GOSALES_TRAIN FETCH FIRST 5 ROWS ONLY\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "From the above sample records, I get a feel for the customer records I will work with. Next, I will ...\n",
      "Metadata: 0.6546600727014\n",
      "\n",
      "Document 3:\n",
      "Content: Additionally, the LINEAR_REGRESSION SP saves the learned values of intercept and the coefficients, along with several other learned parameter values, in a table. The tableâ€™s name takes this form: MODELNAME_MODEL. For the GOSALES_LINREG model, the name of its metadata table is GOSALES_LINREG_MODEL.\n",
      "The following SQL will display the values of the learned model parameters.\n",
      "SELECT VAR_NAME, LEVEL_NAME, VALUE FROM GOSALES.GOSALES_LINREG_MODEL\n",
      "\n",
      "\n",
      "\n",
      "You may have noticed that the above output has more fe...\n",
      "Metadata: 0.6550462902526767\n",
      "\n",
      "Document 4:\n",
      "Content: 3 comments\n",
      "                        \n",
      "\n",
      "                            176 views\n",
      "                        \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            Permalink\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                Comments\n",
      "\n",
      "                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RadosÅ‚aw Matras\n",
      "\n",
      "\n",
      "\n",
      "                                                    Wed December 04, 2024 05:44 AM\n",
      "                                                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "HeyÂ Quader,\n",
      "Sorry for the late answer. I used Db2u in this exercise, so I expected it to work out of the box. Anyway,...\n",
      "Metadata: 0.6688202866782588\n",
      "\n",
      "Document 5:\n",
      "Content: Step-by-Step Guide to Building a Linear Regression Model inside IBM Db2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Â Â \n",
      "                                                    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  Community  \n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  Search Options    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  Search Options    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Log in\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Skip to main content (Press Enter).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Sign in\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Skip auxiliary navigation (Press Enter).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "IBM TechXchange\n",
      "\n",
      "Community\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Cloud Global\n",
      "Cloud Native Apps\n",
      "Cloud Partner Accele...\n",
      "Metadata: 0.6729781068068198\n"
     ]
    }
   ],
   "source": [
    "# query = 'How to train a linear regression model inside Db2?'\n",
    "# query = 'How to see the list of in database ML models in Db2?'\n",
    "# query = 'How to impute missing values of columns in Db2?'\n",
    "# query = 'How to generate predictions using a Python UDF?'\n",
    "# query = 'How to compute summary statistics in Db2 for machine learning?'\n",
    "# query = 'How to generate predictions using a linear regression model?'\n",
    "# query = 'How to drop a model?'\n",
    "# query = 'What is Python UDF?'\n",
    "\n",
    "\n",
    "\n",
    "response = qa_chain.invoke({\"query\": query})\n",
    "\n",
    "# Extract the answer\n",
    "answer = response['result']\n",
    "print(\"Answer:\")\n",
    "display(Markdown(answer))\n",
    "\n",
    "# Extract and print the retrieved documents\n",
    "source_documents = response['source_documents']\n",
    "print(\"\\nRetrieved Contexts:\")\n",
    "for i, doc in enumerate(source_documents, 1):\n",
    "    print(f\"\\nDocument {i}:\")\n",
    "    print(f\"Content: {doc.page_content[:500]}...\")  # Display the first 500 characters\n",
    "    print(f\"Metadata: {doc.metadata['distance']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the connection when done\n",
    "# cleanup_db()\n",
    "# close_db2_connection()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
