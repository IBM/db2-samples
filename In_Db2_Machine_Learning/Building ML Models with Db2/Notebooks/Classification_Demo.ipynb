{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: Building a Decision Tree Classification Model with IBM Db2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents:\n",
    "* [1. Introduction](#Introduction)\n",
    "* [2. Imports](#Imports)\n",
    "* [3. Connect to DB](#Connect-to-DB)\n",
    "* [4. Data Exploration](#EDA)\n",
    "* [5. Splitting Data into Training, Validation, and Test Sets](#Split-Data)\n",
    "* [6. Data Transformation](#Data-Transformation)\n",
    "* [7. Model Training](#Model-Training)\n",
    "* [8. Evaluate Unpruned Model on Training and Test Set](#Eval-UP)\n",
    "* [9. Hyperparameter Tuning](#Hyperparam-Tuning)\n",
    "* [10. Evaluate Pruned Model on Training and Test Set](#Eval-P-Test)\n",
    "* [11. Deployed Model](#Deployment)\n",
    "* [12. Conclusion](#Conclusion)\n",
    "* [13. Cleaning up Model and Tables](#Cleaning-up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction <a class=\"anchor\" id=\"Introduction\"></a>\n",
    "\n",
    "\n",
    "In this notebook, we demonstrate the use of Db2 Stored Procedures in building a Machine Learning Pipeline.\n",
    "\n",
    "\n",
    "We will build a Decision Tree Classifier using the Titanic Dataset. This model will predict which passengers survived the Titanic shipwreck. The dataset contains 891 rows and 12 features (one of which is the target feature).\n",
    "\n",
    "|Variable | Definition | Key|\n",
    "| --- | --- |--- |\n",
    "|survival | Survival | 0 = No, 1 = Yes|\n",
    "|pclass | Ticket class | 1 = 1st, 2 = 2nd, 3 = 3rd|\n",
    "|sex | Sex|\n",
    "|Age | Age in years|\n",
    "|sibsp | # of siblings / spouses aboard the Titanic|\n",
    "|parch | # of parents / children aboard the Titanic|\n",
    "|ticket | Ticket number|\n",
    "|fare | Passenger fare| \n",
    "|cabin | Cabin number|\n",
    "|embarked | Port of Embarkation | C = Cherbourg, Q = Queenstown, S = Southampton|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports <a class=\"anchor\" id=\"Imports\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database connectivity\n",
    "import ibm_db\n",
    "import ibm_db_dbi\n",
    "\n",
    "# Pandas for loading values into memory for later visualization\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "from itertools import combinations_with_replacement\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(12,8)})\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Connect to DB<a class=\"anchor\" id=\"Connect-to-DB\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to DB\n",
    "\n",
    "schema = \"CLASS\"\n",
    "\n",
    "conn_str = \"DATABASE=______;\" + \\\n",
    "           \"HOSTNAME=______;\" + \\\n",
    "           \"PROTOCOL=______;\"  + \\\n",
    "           \"PORT=______;\" + \\\n",
    "           \"UID=______;\" + \\\n",
    "           \"PWD=_______;\"\n",
    "\n",
    "\n",
    "ibm_db_conn = ibm_db.connect(conn_str,\"\",\"\")\n",
    "conn = ibm_db_dbi.Connection(ibm_db_conn)\n",
    "print('Connection to Db2 Instance Created!')\n",
    "rc = ibm_db.close(ibm_db_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for connecting to a particular DB schema\n",
    "def connect_to_schema(schema, conn_str):\n",
    "    \"\"\"Connect to a particular DB schema.\n",
    "    \n",
    "    Input:  schema - name of schema in Db2 to connect to\n",
    "            conn_str - a Db2 connection string\n",
    "    \n",
    "    Output: none\n",
    "    \"\"\"\n",
    "    ibm_db_conn = ibm_db.connect(conn_str,\"\",\"\")\n",
    "    conn = ibm_db_dbi.Connection(ibm_db_conn)\n",
    "\n",
    "    sql = \"set schema \"+schema\n",
    "    stmt = ibm_db.exec_immediate(ibm_db_conn, sql)\n",
    "    \n",
    "    return ibm_db_conn, conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for plotting confusion matrix\n",
    "def plot_conf_mtx(table_name):\n",
    "    \"\"\"Plot a confusion matrix similar in style to sklearn.\n",
    "    \n",
    "    Input:  table_name - Confusion matrix table generated by IDAX.CONFUSION_MATRIX\n",
    "    \n",
    "    Output: Confusion matrix plot\n",
    "    \"\"\"\n",
    "    \n",
    "    ibm_db_conn, conn = connect_to_schema(schema,conn_str)\n",
    "    sql = \"SELECT CNT FROM \" + table_name\n",
    "\n",
    "    arr = pd.read_sql(sql,conn).to_numpy()\n",
    "    conf_matrix=np.vstack( (np.hstack((arr[0],arr[1])),np.hstack((arr[2],arr[3]) ) ))\n",
    "    \n",
    "    group_names = ['True Negatives','False Positives','False Negatives','True Positives']\n",
    "    group_counts = [\"{0:0.0f}\".format(value) for value in conf_matrix.flatten()]\n",
    "    group_percentages = [\"{0:.2%}\".format(value) for value in conf_matrix.flatten()/np.sum(conf_matrix)]\n",
    "    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "    \n",
    "    sns.heatmap(conf_matrix, annot=labels, fmt='', cmap='Blues')\n",
    "    plt.xlabel('Predicted Value')\n",
    "    plt.ylabel('Actual Value')\n",
    "    rc = ibm_db.close(ibm_db_conn)\n",
    "    print('Connection Closed:',rc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom function for collecting statistics from confusion matrix\n",
    "# Function for collecting statistics from confusion matrix\n",
    "def get_conf_mtx_stats(matrixTable):\n",
    "    \"\"\"Get Classification Accuracy, Precision, Recall from a confusion matrix.\n",
    "    \n",
    "    Input:  matrixTable - Confusion matrix table generated by IDAX.CONFUSION_MATRIX\n",
    "    \n",
    "    Output: Classification Accuracy, Precision, Recall\n",
    "    \"\"\"\n",
    "    \n",
    "    ibm_db_conn, conn = connect_to_schema(schema,conn_str)\n",
    "\n",
    "    sql = \"CALL IDAX.CMATRIX_STATS('matrixTable=\"+matrixTable +\"')\"\n",
    "    stmt = ibm_db.exec_immediate(ibm_db_conn, sql)\n",
    "    \n",
    "    \n",
    "    df = pd.DataFrame(columns=['Class', 'Precision', 'Recall'])\n",
    "    \n",
    "    row = ibm_db.fetch_assoc(stmt)\n",
    "    while row != False :\n",
    "        print(\"Classification Accuracy: \",\"{:.1%}\".format(row[\"ACC\"]))\n",
    "        row = ibm_db.fetch_assoc(stmt)\n",
    "\n",
    "    stmt1 = ibm_db.next_result(stmt)\n",
    "    while stmt1 != False:\n",
    "        row = ibm_db.fetch_assoc(stmt1)\n",
    "        while row != False :\n",
    "            to_append=[row[\"CLASS\"],\"{:.1%}\".format(row[\"PPV\"]),\"{:.1%}\".format(row[\"TPR\"])]\n",
    "            df_length = len(df)\n",
    "            df.loc[df_length] = to_append\n",
    "            row = ibm_db.fetch_assoc(stmt1)\n",
    "        stmt1 = ibm_db.next_result(stmt)\n",
    "    display(df)\n",
    "    rc = ibm_db.close(ibm_db_conn)\n",
    "    print('Connection Closed:',rc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for plotting CDFs from runstats statistics\n",
    "def plot_cdf_from_runstats_quartiles(col_name,quartiles_df,percentiles=True,norm_cdf=True):\n",
    "    \n",
    "    \"\"\"Plot feature CDF from quartiles statistics from Db2 RUNSTATS.\n",
    "    \n",
    "    Input:  col_name - Name of the feature\n",
    "            quartiles_df - DataFrame from RUNSTATS with Type = 'Q'\n",
    "            percentiles=True - Plot 25th, 50th, 75th percentiles\n",
    "            norm_cdf=True - Plot CDF of feature normally distributed over its range\n",
    "    \n",
    "    Output: Plots feature CDF\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    COL_HISTOGRAM = quartiles_df[(quartiles_df[\"COLNAME\"] == col_name)].copy()\n",
    "    COL_HISTOGRAM[\"VAL_PERCENT\"]=COL_HISTOGRAM[\"VALCOUNT\"]/891\n",
    "    \n",
    "    sns.lineplot(x='COLVALUE', y='VAL_PERCENT', data=COL_HISTOGRAM, label=\"CDF\", lw=2, err_style=None)\n",
    "    \n",
    "    # Plot dashed lines indicating points of 25th, 50th, and 75th percentiles\n",
    "    if percentiles:\n",
    "        plt.hlines(y=0.25, xmin=0, xmax=COL_HISTOGRAM.iloc[(COL_HISTOGRAM['VAL_PERCENT']-0.25).abs().argsort()[:1]]['COLVALUE'], colors='r', linestyles='--',label='25th Percentile')\n",
    "        plt.vlines(x=COL_HISTOGRAM.iloc[(COL_HISTOGRAM['VAL_PERCENT']-0.25).abs().argsort()[:1]]['COLVALUE'], ymin=0, ymax=0.25, colors='r', linestyles='--',label='_nolegend_')\n",
    "    \n",
    "        plt.hlines(y=0.50, xmin=0, xmax=COL_HISTOGRAM.iloc[(COL_HISTOGRAM['VAL_PERCENT']-0.50).abs().argsort()[:1]]['COLVALUE'], colors='g', linestyles='--',label='50th Percentile')\n",
    "        plt.vlines(x=COL_HISTOGRAM.iloc[(COL_HISTOGRAM['VAL_PERCENT']-0.50).abs().argsort()[:1]]['COLVALUE'], ymin=0, ymax=0.50, colors='g', linestyles='--',label='_nolegend_')\n",
    "    \n",
    "        plt.hlines(y=0.75, xmin=0, xmax=COL_HISTOGRAM.iloc[(COL_HISTOGRAM['VAL_PERCENT']-0.75).abs().argsort()[:1]]['COLVALUE'], colors='k', linestyles='--',label='75th Percentile')\n",
    "        plt.vlines(x=COL_HISTOGRAM.iloc[(COL_HISTOGRAM['VAL_PERCENT']-0.75).abs().argsort()[:1]]['COLVALUE'], ymin=0, ymax=0.75, colors='k', linestyles='--',label='_nolegend_')\n",
    "    \n",
    "    # Plot CDF of normal distribution where mean = RANGE/2, with same stddev as original distribution\n",
    "    if norm_cdf:\n",
    "        x = np.linspace(col_prop[col_prop['NAME']==col_name]['MINIMUM'],col_prop[col_prop['NAME']==col_name]['MAXIMUM'],100)\n",
    "        mu = (col_prop[col_prop['NAME']==col_name]['MAXIMUM']-col_prop[col_prop['NAME']==col_name]['MINIMUM'])/2\n",
    "        sigma = np.sqrt(col_prop[col_prop['NAME']==col_name]['VARIANCE'])\n",
    "        \n",
    "        y = ss.norm.cdf(x, mu, sigma)\n",
    "        plt.plot(x, y, label='Normal Distribution CDF', color='#FF4500',linestyle='--')\n",
    "        \n",
    "    plt.title(col_name)\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Cumulative Probability')\n",
    "    plt.ylim(bottom=0)\n",
    "    plt.xlim(left=0)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correlation matrix from Db2 Stats\n",
    "def create_correlation_matrix(cont_col_list,table_name,ibm_db_conn):\n",
    "    \"\"\"Create a correlation matrix from a list of continuous features\n",
    "    \n",
    "    Input: cont_col_list - A list of features (e.g. ['AGE','FARE',...])\n",
    "    \n",
    "    Output: Plots a correlation matrix\n",
    "    \"\"\"\n",
    "\n",
    "    corr_mtx = pd.DataFrame(columns=cont_col_list, index=cont_col_list)\n",
    "\n",
    "    for combo in combinations_with_replacement(cont_cols, 2):\n",
    "\n",
    "        col1=combo[0]\n",
    "        col2=combo[1]\n",
    "\n",
    "\n",
    "        sql = \"SELECT CORRELATION(\"+col1+\",\"+col2+\") FROM \"+table_name\n",
    "        stmt = ibm_db.exec_immediate(ibm_db_conn, sql)\n",
    "        row = ibm_db.fetch_assoc(stmt)\n",
    "\n",
    "\n",
    "        to_append=\"{:.2f}\".format(row['1'])\n",
    "        corr_mtx.loc[col1,col2] = to_append\n",
    "        corr_mtx.loc[col2,col1] = to_append\n",
    "        \n",
    "    corr_mtx=corr_mtx.astype(float)\n",
    "    sns.heatmap(corr_mtx, annot=True, fmt = \"0.2f\",vmin=-1, vmax=1, center= 0, cmap= 'RdBu');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Exploration<a class=\"anchor\" id=\"EDA\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without moving data from Db2, we will use built-in Db2 functions and stored procedures to perform basic statistical analysis of our entire dataset, and visualize the results to answer the following questions:\n",
    "- Are there any missing values?\n",
    "- What is the underlying distribution of numerical features?\n",
    "- How many unique values for categorical features?\n",
    "- Are any features correlated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collect column statistics on the entire dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect column statistics\n",
    "ibm_db_conn, conn = connect_to_schema(schema,conn_str)\n",
    "\n",
    "sql = \"CALL IDAX.COLUMN_PROPERTIES('intable=DATA.TITANIC , outtable=T_COL_PROP, \"\n",
    "sql+= \"withstatistics=true, incolumn=PASSENGERID:id; SURVIVED:target')\"\n",
    "stmt = ibm_db.exec_immediate(ibm_db_conn, sql)\n",
    "\n",
    "sql = \"SELECT * FROM T_COL_PROP\"\n",
    "col_prop = pd.read_sql(sql,conn)\n",
    "\n",
    "rc = ibm_db.close(ibm_db_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the column properties\n",
    "col_prop.sort_values('COLNO')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**: Cardinality = Number of distinct values\n",
    "- NAME, TICKET have too many distinct values to be useful. Can be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify Columns with missing values\n",
    "ibm_db_conn, conn = connect_to_schema(schema,conn_str)\n",
    "\n",
    "sql = \"SELECT COLNO, NAME, TYPE,NUMMISSING,NUMMISSING+NUMINVALID+NUMVALID as NUMBER_OF_VALUES, \"\n",
    "sql+= \"ROUND(dec(NUMMISSING,10,2)/(dec(NUMMISSING, 10,2)+dec(NUMINVALID, 10,2)+dec(NUMVALID, 10,2))*100,2) as PERCENT_NULL \"\n",
    "sql+= \"from T_COL_PROP where NUMMISSING > 0 order by PERCENT_NULL DESC\"\n",
    "missing_vals = pd.read_sql(sql,conn)\n",
    "\n",
    "rc = ibm_db.close(ibm_db_conn)\n",
    "\n",
    "missing_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**:\n",
    "- CABIN has 77% missing values\n",
    "- AGE and EMBARKED require missing value imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot CDFs of continuous features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call runstats on continuous columns\n",
    "ibm_db_conn, conn = connect_to_schema(schema,conn_str)\n",
    "\n",
    "sql = \"CALL sysproc.admin_cmd('runstats on table  DATA.TITANIC \"\n",
    "sql+= \"\"\"with distribution on columns (\"AGE\",\"FARE\") default num_quantiles 50');\"\"\"\n",
    "stmt = ibm_db.exec_immediate(ibm_db_conn, sql)\n",
    "\n",
    "rc = ibm_db.close(ibm_db_conn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Quartiles from runstats results\n",
    "ibm_db_conn, conn = connect_to_schema(schema,conn_str)\n",
    "\n",
    "sql = \"select * from SYSSTAT.COLDIST where TABSCHEMA = 'DATA' and TABNAME = 'TITANIC' \"\n",
    "sql+= \"and TYPE = 'Q'\"\n",
    "quartiles_df = pd.read_sql(sql,conn)\n",
    "\n",
    "rc = ibm_db.close(ibm_db_conn)\n",
    "\n",
    "# Convert values to numeric\n",
    "quartiles_df.COLVALUE = pd.to_numeric(quartiles_df.COLVALUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot CDFs using quartile statistics\n",
    "for col_name in [\"AGE\",\"FARE\"]:\n",
    "    plot_cdf_from_runstats_quartiles(col_name,quartiles_df, percentiles=True,norm_cdf=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "- AGE is well distributed, but takes a wide range of values. This feature should be zero-mean normalized. This CDF also confirms that 20% of its values are missing\n",
    "- FARE is heavily skewed. Standardization/normalization will not help. This feature should be binned into a discrete categorical feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Countplots for categorical features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runstats on nominal columns\n",
    "ibm_db_conn, conn = connect_to_schema(schema,conn_str)\n",
    "\n",
    "sql = \"CALL sysproc.admin_cmd('runstats on table DATA.TITANIC \"\n",
    "sql+= \"\"\"with distribution on columns (\"SURVIVED\",\"PCLASS\",\"SEX\",\"EMBARKED\",\"SIBSP\",\"PARCH\") default num_freqvalues 10');\"\"\"\n",
    "\n",
    "stmt = ibm_db.exec_immediate(ibm_db_conn, sql)\n",
    "\n",
    "rc = ibm_db.close(ibm_db_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load runstats results\n",
    "ibm_db_conn, conn = connect_to_schema(schema,conn_str)\n",
    "\n",
    "sql = \"select * from SYSSTAT.COLDIST where TABSCHEMA = 'DATA' and TABNAME = 'TITANIC' \"\n",
    "sql+= \"and TYPE = 'F'\"\n",
    "FREQ_VALUES = pd.read_sql(sql,conn)\n",
    "\n",
    "rc = ibm_db.close(ibm_db_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countplot\n",
    "_,ax = plt.subplots(3,2, figsize=(20,20))\n",
    "nom_cols=[\"SURVIVED\",\"PCLASS\",\"SEX\",\"EMBARKED\",\"SIBSP\",\"PARCH\"]\n",
    "count=0\n",
    "for i in range(3):\n",
    "    for j in range(2):\n",
    "        COL_FREQ_VALUES = FREQ_VALUES.loc[(FREQ_VALUES.COLNAME == nom_cols[count]) & (FREQ_VALUES.VALCOUNT != -1)]\n",
    "        sns.barplot(x='COLVALUE', y='VALCOUNT', data=COL_FREQ_VALUES,ax=ax[i,j])\n",
    "        ax[i,j].set_title(nom_cols[count])\n",
    "        ax[i,j].set(xlabel='Value')\n",
    "        ax[i,j].set(ylabel='Count')\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**:\n",
    "- As expected:\n",
    "    - most passengers did not survive.\n",
    "    - there were more males than females\n",
    "    - most passengers were in 3rd class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature correlation matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "ibm_db_conn, conn = connect_to_schema(schema,conn_str)\n",
    "\n",
    "table_name = \"DATA.TITANIC\"\n",
    "cont_cols = [\"PASSENGERID\", \"SURVIVED\", \"PCLASS\", \"AGE\",\"SIBSP\",\"PARCH\",\"FARE\"]\n",
    "create_correlation_matrix(cont_cols,table_name,ibm_db_conn)\n",
    "    \n",
    "rc = ibm_db.close(ibm_db_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "- As expected, passenger class is negatively correlated with survival as many of the passengers who did not survive were in 3rd class.\n",
    "- Age is negatively correlated as most of the survivors included young children"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Splitting Data into Training, Validation, and Test Set<a class=\"anchor\" id=\"Split-Data\"></a>\n",
    "\n",
    "We first create a view of the original data table, but without the features TICKET, CABIN, and NAME. This will simulate dropping these features without altering the raw data table.\n",
    "\n",
    "We will then split the data into a training, validation, and testing set.\n",
    "\n",
    "We create the following tables:\n",
    "\n",
    "- T_TRAIN contains 64% of data - used for model training\n",
    "- T_VAL contains 16% of data - used for model tuning (pruning)\n",
    "- T_TEST contains 20% of data - used for model testing and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a View. This simulates dropping columns from the raw data table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create view without TICKET, CABIN, and NAME features\n",
    "ibm_db_conn, conn = connect_to_schema(schema,conn_str)\n",
    "\n",
    "sql= \"CREATE VIEW TITANIC_VIEW AS SELECT PASSENGERID, SURVIVED, PCLASS, SEX, AGE, SIBSP, PARCH, FARE, EMBARKED FROM DATA.TITANIC\"\n",
    "stmt = ibm_db.exec_immediate(ibm_db_conn, sql)\n",
    "\n",
    "rc = ibm_db.close(ibm_db_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train-test split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "ibm_db_conn, conn = connect_to_schema(schema,conn_str)\n",
    "\n",
    "sql= \"CALL IDAX.SPLIT_DATA('intable=TITANIC_VIEW, id=PASSENGERID, traintable=T_TRAIN_FULL, testtable=T_TEST, fraction=0.8, seed=1')\"\n",
    "stmt = ibm_db.exec_immediate(ibm_db_conn, sql)\n",
    "\n",
    "rc = ibm_db.close(ibm_db_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train-val split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-val split\n",
    "ibm_db_conn, conn = connect_to_schema(schema,conn_str)\n",
    "\n",
    "sql= \"\"\"CALL IDAX.SPLIT_DATA('intable=T_TRAIN_FULL, \n",
    "        id=PASSENGERID, traintable=T_TRAIN, testtable=T_VAL, \n",
    "        fraction=0.8, seed=1')\"\"\"\n",
    "stmt = ibm_db.exec_immediate(ibm_db_conn, sql)\n",
    "\n",
    "# Drop T_TRAIN_FULL as it is no longer needed\n",
    "sql= \"\"\"DROP TABLE T_TRAIN_FULL\"\"\"\n",
    "stmt = ibm_db.exec_immediate(ibm_db_conn, sql)\n",
    "\n",
    "rc = ibm_db.close(ibm_db_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Transformation<a class=\"anchor\" id=\"Data-Transformation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Available Data Transformations:**\n",
    "\n",
    "- Impute Data (median, mean, most frequent, replace)\n",
    "- Column Standardization: (x-mean)/stddev [-inf,+inf]\n",
    "- Column Normalization: min-max scaling [0,1]\n",
    "- Column Discretization\n",
    "\n",
    "**Data Transformation Steps:**\n",
    "1. Drop CABIN, TICKET, NAME features. These features are not relevant in predicting passenger survival. Done by creating a view in section 5\n",
    "2. Data imputation - [Age] = mean, [EMARKED] ='S' --> most frequent value\n",
    "3. Standardize [AGE]\n",
    "4. Discretize [FARE] into bins of equal frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collect statistics on training set, to be used for transforming validation and test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create T_STATS table that contains training dataset feature stats (mean, stdev, freq, etc) used for data \n",
    "# transformation in T_VAL, T_TEST\n",
    "ibm_db_conn, conn = connect_to_schema(schema,conn_str)\n",
    "\n",
    "sql = \"CALL IDAX.SUMMARY1000('intable=T_TRAIN,outtable=T_TRAIN_STATS')\"\n",
    "stmt = ibm_db.exec_immediate(ibm_db_conn, sql)\n",
    "\n",
    "rc = ibm_db.close(ibm_db_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Missing Value Imputation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values imputation for T_TRAIN\n",
    "ibm_db_conn, conn = connect_to_schema(schema,conn_str)\n",
    "\n",
    "sql = \"CALL IDAX.IMPUTE_DATA('intable=T_TRAIN,method=mean,inColumn=AGE')\"\n",
    "stmt = ibm_db.exec_immediate(ibm_db_conn, sql)\n",
    "\n",
    "sql = \"CALL IDAX.IMPUTE_DATA('intable=T_TRAIN,method=replace,nominalValue=S,inColumn=EMBARKED')\"\n",
    "stmt = ibm_db.exec_immediate(ibm_db_conn, sql)\n",
    "\n",
    "rc = ibm_db.close(ibm_db_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values imputation for T_VAL, T_TEST\n",
    "ibm_db_conn, conn = connect_to_schema(schema,conn_str)\n",
    "\n",
    "\n",
    "sql = \"\"\"UPDATE T_VAL\n",
    "         SET AGE = (SELECT AVERAGE FROM T_TRAIN_STATS_NUM WHERE COLUMNNAME='AGE')\n",
    "         WHERE AGE IS NULL\"\"\"\n",
    "stmt = ibm_db.exec_immediate(ibm_db_conn, sql)\n",
    "\n",
    "sql = \"\"\"UPDATE T_VAL\n",
    "         SET EMBARKED = (SELECT MOSTFREQUENTVALUE FROM T_TRAIN_STATS_CHAR WHERE COLNAME='EMBARKED')\n",
    "         WHERE EMBARKED IS NULL\"\"\"\n",
    "stmt = ibm_db.exec_immediate(ibm_db_conn, sql)\n",
    "\n",
    "sql = \"\"\"UPDATE T_TEST\n",
    "         SET AGE = (SELECT AVERAGE FROM T_TRAIN_STATS_NUM WHERE COLUMNNAME='AGE')\n",
    "         WHERE AGE IS NULL\"\"\"\n",
    "stmt = ibm_db.exec_immediate(ibm_db_conn, sql)\n",
    "\n",
    "sql = \"\"\"UPDATE T_TEST\n",
    "         SET EMBARKED = (SELECT MOSTFREQUENTVALUE FROM T_TRAIN_STATS_CHAR WHERE COLNAME='EMBARKED')\n",
    "         WHERE EMBARKED IS NULL\"\"\"\n",
    "stmt = ibm_db.exec_immediate(ibm_db_conn, sql)\n",
    "\n",
    "rc = ibm_db.close(ibm_db_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Standardization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in data for later comparison\n",
    "ibm_db_conn, conn = connect_to_schema(schema,conn_str)\n",
    "\n",
    "sql = \"SELECT AGE FROM T_TRAIN ORDER BY RAND() FETCH FIRST 200 ROWS ONLY\"\n",
    "train = pd.read_sql(sql,conn)\n",
    "\n",
    "sql = \"SELECT AGE FROM T_VAL ORDER BY RAND() FETCH FIRST 200 ROWS ONLY\"\n",
    "val = pd.read_sql(sql,conn)\n",
    "\n",
    "sql = \"SELECT AGE FROM T_TEST ORDER BY RAND() FETCH FIRST 200 ROWS ONLY\"\n",
    "test = pd.read_sql(sql,conn)\n",
    "\n",
    "rc = ibm_db.close(ibm_db_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization for T_TRAIN, write to _NORMED\n",
    "\n",
    "ibm_db_conn, conn = connect_to_schema(schema,conn_str)\n",
    "\n",
    "sql = \"\"\"CALL IDAX.STD_NORM('intable=T_TRAIN, id=PASSENGERID,\n",
    "        inColumn=SURVIVED:L;PCLASS:L;SEX:L;SIBSP:L;PARCH:L;EMBARKED:L;AGE:S;FARE:L,\n",
    "        outtable=T_TRAIN_NORMED')\"\"\"\n",
    "stmt = ibm_db.exec_immediate(ibm_db_conn, sql)\n",
    "\n",
    "# Drop T_TRAIN\n",
    "sql= \"\"\"DROP TABLE T_TRAIN\"\"\"\n",
    "stmt = ibm_db.exec_immediate(ibm_db_conn, sql)\n",
    "\n",
    "rc = ibm_db.close(ibm_db_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AGE Standardization for T_VAL, T_TEST\n",
    "ibm_db_conn, conn = connect_to_schema(schema,conn_str)\n",
    "\n",
    "sql = \"\"\"UPDATE T_VAL\n",
    "         SET AGE = ((CAST(AGE AS FLOAT) - (SELECT AVERAGE FROM T_TRAIN_STATS_NUM WHERE COLUMNNAME='AGE'))/(SELECT STDDEV FROM T_TRAIN_STATS_NUM WHERE COLUMNNAME='AGE'))\"\"\"\n",
    "stmt = ibm_db.exec_immediate(ibm_db_conn, sql)\n",
    "\n",
    "\n",
    "sql = \"\"\"UPDATE T_TEST\n",
    "         SET AGE = ((CAST(AGE AS FLOAT) - (SELECT AVERAGE FROM T_TRAIN_STATS_NUM WHERE COLUMNNAME='AGE'))/(SELECT STDDEV FROM T_TRAIN_STATS_NUM WHERE COLUMNNAME='AGE'))\"\"\"\n",
    "stmt = ibm_db.exec_immediate(ibm_db_conn, sql)\n",
    "\n",
    "# Rename AGE to STD_AGE for later comparison with T_TRAIN\n",
    "\n",
    "sql= \"\"\"ALTER TABLE T_VAL RENAME COLUMN AGE TO STD_AGE\"\"\"\n",
    "stmt = ibm_db.exec_immediate(ibm_db_conn, sql)\n",
    "\n",
    "sql= \"\"\"ALTER TABLE T_TEST RENAME COLUMN AGE TO STD_AGE\"\"\"\n",
    "stmt = ibm_db.exec_immediate(ibm_db_conn, sql)\n",
    "\n",
    "rc = ibm_db.close(ibm_db_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify standardization was applied properly\n",
    "ibm_db_conn, conn = connect_to_schema(schema,conn_str)\n",
    "\n",
    "sql = \"SELECT STD_AGE FROM T_TRAIN_NORMED ORDER BY RAND() FETCH FIRST 200 ROWS ONLY\"\n",
    "train_standardized = pd.read_sql(sql,conn)\n",
    "\n",
    "sql = \"SELECT STD_AGE FROM T_VAL ORDER BY RAND() FETCH FIRST 200 ROWS ONLY\"\n",
    "val_standardized = pd.read_sql(sql,conn)\n",
    "\n",
    "sql = \"SELECT STD_AGE FROM T_TEST ORDER BY RAND() FETCH FIRST 200 ROWS ONLY\"\n",
    "test_standardized = pd.read_sql(sql,conn)\n",
    "\n",
    "rc = ibm_db.close(ibm_db_conn)\n",
    "\n",
    "# Plot Age and Fare\n",
    "_,ax = plt.subplots(3,2, figsize=(20,20))\n",
    "\n",
    "train['AGE'].hist(ax=ax[0,0])\n",
    "ax[0,0].set(xlabel='TRAIN AGE')\n",
    "ax[0,0].set(ylabel='Count')\n",
    "\n",
    "train_standardized['STD_AGE'].hist(ax=ax[0,1])\n",
    "ax[0,1].set(xlabel='TRAIN STD_AGE')\n",
    "ax[0,1].set(ylabel='Count')\n",
    "\n",
    "val['AGE'].hist(ax=ax[1,0])\n",
    "ax[1,0].set(xlabel='VAL AGE')\n",
    "ax[1,0].set(ylabel='Count')\n",
    "\n",
    "val_standardized['STD_AGE'].hist(ax=ax[1,1])\n",
    "ax[1,1].set(xlabel='VAL STD_AGE')\n",
    "ax[1,1].set(ylabel='Count')\n",
    "\n",
    "test['AGE'].hist(ax=ax[2,0])\n",
    "ax[2,0].set(xlabel='TEST AGE')\n",
    "ax[2,0].set(ylabel='Count')\n",
    "\n",
    "test_standardized['STD_AGE'].hist(ax=ax[2,1])\n",
    "ax[2,1].set(xlabel='TEST STD_AGE')\n",
    "ax[2,1].set(ylabel='Count');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** The values of the AGE feature have been standardized to have zero mean, and now take on a smaller range of values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Discretization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in data for later comparison\n",
    "ibm_db_conn, conn = connect_to_schema(schema,conn_str)\n",
    "\n",
    "sql = \"SELECT FARE FROM T_TRAIN_NORMED ORDER BY RAND() FETCH FIRST 200 ROWS ONLY\"\n",
    "train = pd.read_sql(sql,conn)\n",
    "\n",
    "sql = \"SELECT FARE FROM T_VAL ORDER BY RAND() FETCH FIRST 200 ROWS ONLY\"\n",
    "val = pd.read_sql(sql,conn)\n",
    "\n",
    "sql = \"SELECT FARE FROM T_TEST ORDER BY RAND() FETCH FIRST 200 ROWS ONLY\"\n",
    "test = pd.read_sql(sql,conn)\n",
    "\n",
    "rc = ibm_db.close(ibm_db_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bins\n",
    "ibm_db_conn, conn = connect_to_schema(schema,conn_str)\n",
    "\n",
    "sql = \"\"\"CALL IDAX.EFDISC('intable=T_TRAIN_NORMED,\n",
    "        inColumn=FARE,bins=5,\n",
    "        outtable=T_BTABLE')\"\"\"\n",
    "stmt = ibm_db.exec_immediate(ibm_db_conn, sql)\n",
    "\n",
    "\n",
    "rc = ibm_db.close(ibm_db_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply discretization\n",
    "ibm_db_conn, conn = connect_to_schema(schema,conn_str)\n",
    "\n",
    "sql = \"\"\"CALL IDAX.APPLY_DISC('intable=T_TRAIN_NORMED, btable=T_BTABLE,\n",
    "        outtable=T_TRAIN_CLEANED')\"\"\"\n",
    "stmt = ibm_db.exec_immediate(ibm_db_conn, sql)\n",
    "\n",
    "sql = \"\"\"CALL IDAX.APPLY_DISC('intable=T_VAL, btable=T_BTABLE,\n",
    "        outtable=T_VAL_CLEANED')\"\"\"\n",
    "stmt = ibm_db.exec_immediate(ibm_db_conn, sql)\n",
    "\n",
    "sql = \"\"\"CALL IDAX.APPLY_DISC('intable=T_TEST, btable=T_BTABLE,\n",
    "        outtable=T_TEST_CLEANED')\"\"\"\n",
    "stmt = ibm_db.exec_immediate(ibm_db_conn, sql)\n",
    "\n",
    "# Drop T_TRAIN_NORMED, T_VAL, T_TEST\n",
    "\n",
    "sql= \"\"\"DROP TABLE T_TRAIN_NORMED\"\"\"\n",
    "stmt = ibm_db.exec_immediate(ibm_db_conn, sql)\n",
    "\n",
    "sql= \"\"\"DROP TABLE T_VAL\"\"\"\n",
    "stmt = ibm_db.exec_immediate(ibm_db_conn, sql)\n",
    "\n",
    "sql= \"\"\"DROP TABLE T_TEST\"\"\"\n",
    "stmt = ibm_db.exec_immediate(ibm_db_conn, sql)\n",
    "\n",
    "rc = ibm_db.close(ibm_db_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify all transformations were applied properly\n",
    "ibm_db_conn, conn = connect_to_schema(schema,conn_str)\n",
    "\n",
    "sql = \"SELECT FARE FROM T_TRAIN_CLEANED\"\n",
    "train_cleaned = pd.read_sql(sql,conn)\n",
    "\n",
    "sql = \"SELECT FARE FROM T_VAL_CLEANED\"\n",
    "val_cleaned = pd.read_sql(sql,conn)\n",
    "\n",
    "sql = \"SELECT FARE FROM T_TEST_CLEANED\"\n",
    "test_cleaned = pd.read_sql(sql,conn)\n",
    "\n",
    "# Get labels for Fare Bins\n",
    "sql = \"SELECT * FROM T_BTABLE\"\n",
    "df=pd.read_sql(sql,conn)\n",
    "L=df[\"BREAK\"].tolist()\n",
    "labels = [\"%.2f\" % member for member in L]\n",
    "\n",
    "rc = ibm_db.close(ibm_db_conn)\n",
    "\n",
    "# Plot Age and Fare\n",
    "_,ax = plt.subplots(3,2, figsize=(20,20))\n",
    "\n",
    "train['FARE'].hist(ax=ax[0,0])\n",
    "ax[0,0].set(xlabel='TRAIN FARE')\n",
    "ax[0,0].set(ylabel='Count')\n",
    "\n",
    "sns.countplot(x='FARE', data=train_cleaned, ax=ax[0,1])\n",
    "ax[0,1].set(xlabel='TRAIN FARE')\n",
    "ax[0,1].set(ylabel='Count')\n",
    "ax[0,1].set_xticklabels(['<'+labels[0],'['+labels[0]+','+labels[1]+']','['+labels[1]+','+labels[2]+']','['+labels[2]+','+labels[3]+']','>'+labels[3]])\n",
    "\n",
    "val['FARE'].hist(ax=ax[1,0])\n",
    "ax[1,0].set(xlabel='VAL FARE')\n",
    "ax[1,0].set(ylabel='Count')\n",
    "\n",
    "sns.countplot(x='FARE', data=val_cleaned, ax=ax[1,1])\n",
    "ax[1,1].set(xlabel='VAL FARE')\n",
    "ax[1,1].set(ylabel='Count')\n",
    "ax[1,1].set_xticklabels(['<'+labels[0],'['+labels[0]+','+labels[1]+']','['+labels[1]+','+labels[2]+']','['+labels[2]+','+labels[3]+']','>'+labels[3]])\n",
    "\n",
    "test['FARE'].hist(ax=ax[2,0])\n",
    "ax[2,0].set(xlabel='TEST AGE')\n",
    "ax[2,0].set(ylabel='Count')\n",
    "\n",
    "sns.countplot(x='FARE', data=test_cleaned, ax=ax[2,1])\n",
    "ax[2,1].set(xlabel='TEST FARE')\n",
    "ax[2,1].set(ylabel='Count')\n",
    "ax[2,1].set_xticklabels(['<'+labels[0],'['+labels[0]+','+labels[1]+']','['+labels[1]+','+labels[2]+']','['+labels[2]+','+labels[3]+']','>'+labels[3]]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** The FARE feature that used to be heavily skewed to the left is now binned into a categorical feature that takes on discrete values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Training<a class=\"anchor\" id=\"Model-Training\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a Decision Tree Classifier to make predictions on our data. We will use only default values for building the model, and then prune the model in section 9. For each model (pruned and unpruned) we will evaluate the model's accuracy on both the training and test set and compare performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "ibm_db_conn, conn = connect_to_schema(schema,conn_str)\n",
    "\n",
    "sql = \"\"\"CALL IDAX.GROW_DECTREE('model=titanic_dt, intable=T_TRAIN_CLEANED, \n",
    "        id=PASSENGERID, target=SURVIVED, incolumn=SURVIVED:nom;PCLASS:nom')\"\"\"\n",
    "stmt = ibm_db.exec_immediate(ibm_db_conn, sql)\n",
    "\n",
    "rc = ibm_db.close(ibm_db_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Return Model Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Return Model Parameters\n",
    "ibm_db_conn, conn = connect_to_schema(schema,conn_str)\n",
    "\n",
    "sql = \"\"\"SELECT * FROM \n",
    "        TITANIC_DT_MODEL\"\"\"\n",
    "dt_model = pd.read_sql(sql,conn)\n",
    "\n",
    "sql = \"\"\"SELECT * FROM \n",
    "        TITANIC_DT_COLUMNS\"\"\"\n",
    "dt_cols = pd.read_sql(sql,conn)\n",
    "\n",
    "rc = ibm_db.close(ibm_db_conn)\n",
    "print('Model Hyperparameters')\n",
    "display(dt_model)\n",
    "print('Model Input Features and Feature Importance')\n",
    "display(dt_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluate Unpruned Model on Training and Test Set<a class=\"anchor\" id=\"Eval-UP\"></a>\n",
    "\n",
    "We evaluate the model on each set by computing a confusion matrix and calculating the following important metrics:\n",
    "- Classification Accuracy\n",
    "- Recall = TPR (True Positive Rate)\n",
    "- Precision = PPV (Positive Predicitive Value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predict on Training and Test Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on Training and Test Set\n",
    "ibm_db_conn, conn = connect_to_schema(schema,conn_str)\n",
    "\n",
    "sql = \"\"\"CALL IDAX.PREDICT_DECTREE('model=titanic_dt, intable=T_TRAIN_CLEANED, \n",
    "        outtable=T_DT_UP_TRAIN_PREDICTIONS')\"\"\"\n",
    "stmt = ibm_db.exec_immediate(ibm_db_conn, sql)\n",
    "\n",
    "sql = \"\"\"CALL IDAX.PREDICT_DECTREE('model=titanic_dt, intable=T_TEST_CLEANED, \n",
    "        outtable=T_DT_UP_TEST_PREDICTIONS')\"\"\"\n",
    "stmt = ibm_db.exec_immediate(ibm_db_conn, sql)\n",
    "\n",
    "rc = ibm_db.close(ibm_db_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate Model Performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrices\n",
    "ibm_db_conn, conn = connect_to_schema(schema,conn_str)\n",
    "\n",
    "sql = \"\"\"CALL IDAX.CONFUSION_MATRIX('intable=T_TRAIN_CLEANED, id=PASSENGERID, target=SURVIVED, \n",
    "        resulttable=T_DT_UP_TRAIN_PREDICTIONS, matrixtable=T_DT_UP_TRAIN_CM');\"\"\"\n",
    "stmt = ibm_db.exec_immediate(ibm_db_conn, sql)\n",
    "\n",
    "\n",
    "sql = \"\"\"CALL IDAX.CONFUSION_MATRIX('intable=T_TEST_CLEANED, id=PASSENGERID, target=SURVIVED, \n",
    "        resulttable=T_DT_UP_TEST_PREDICTIONS, matrixtable=T_DT_UP_TEST_CM');\"\"\"\n",
    "stmt = ibm_db.exec_immediate(ibm_db_conn, sql)\n",
    "\n",
    "rc = ibm_db.close(ibm_db_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print confusion matrices\n",
    "print('Unpruned model on Training Set:')\n",
    "plot_conf_mtx('T_DT_UP_TRAIN_CM')\n",
    "plt.show()\n",
    "print('Unpruned model on Test Set:')\n",
    "plot_conf_mtx('T_DT_UP_TEST_CM')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix Statistics for unpruned tree on training and test sets\n",
    "\n",
    "print('Unpruned Model on Training Set:')\n",
    "get_conf_mtx_stats('T_DT_UP_TRAIN_CM')\n",
    "print('Unpruned Model on Test Set:')\n",
    "get_conf_mtx_stats('T_DT_UP_TEST_CM')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "Unpruned model performance:\n",
    "\n",
    "- Unpruned Training Accuracy: 84.4%\n",
    "- Unpruned Test Accuracy: 76.4%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Hyperparameter Tuning<a class=\"anchor\" id=\"Hyperparam-Tuning\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we prune the decision tree using the .PRUNE_DECTREE() function. This function works by pruning the tree from the bottom up and comparing the improvement in classification accuracy. Classification accuracy is measured on the validation set. This process continues until the accuracy of the model no longer improves.\n",
    "\n",
    "If pruning works as expected, we should see a performance improvement on the test set and a performance decrease on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prune Tree\n",
    "ibm_db_conn, conn = connect_to_schema(schema,conn_str)\n",
    "\n",
    "sql = \"\"\"CALL IDAX.PRUNE_DECTREE('model=titanic_dt, valtable=T_VAL_CLEANED')\"\"\"\n",
    "stmt = ibm_db.exec_immediate(ibm_db_conn, sql)\n",
    "\n",
    "rc = ibm_db.close(ibm_db_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Investigate pruned model parameters\n",
    "ibm_db_conn, conn = connect_to_schema(schema,conn_str)\n",
    "\n",
    "sql = \"\"\"SELECT * FROM \n",
    "        TITANIC_DT_MODEL\"\"\"\n",
    "dt_model = pd.read_sql(sql,conn)\n",
    "\n",
    "rc = ibm_db.close(ibm_db_conn)\n",
    "dt_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:** We have decreased the depth of the tree by 1 and removed 10 nodes in the pruning process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluate Pruned Model on Training and Test Set<a class=\"anchor\" id=\"Eval-P-Test\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predict with Pruned Model on Training and Test Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict again on Training and Test Set\n",
    "ibm_db_conn, conn = connect_to_schema(schema,conn_str)\n",
    "\n",
    "sql = \"\"\"CALL IDAX.PREDICT_DECTREE('model=titanic_dt, intable=T_TRAIN_CLEANED, \n",
    "        outtable=T_DT_P_TRAIN_PREDICTIONS')\"\"\"\n",
    "stmt = ibm_db.exec_immediate(ibm_db_conn, sql)\n",
    "\n",
    "sql = \"\"\"CALL IDAX.PREDICT_DECTREE('model=titanic_dt, intable=T_TEST_CLEANED,\n",
    "        outtable=T_DT_P_TEST_PREDICTIONS')\"\"\"\n",
    "stmt = ibm_db.exec_immediate(ibm_db_conn, sql)\n",
    "\n",
    "rc = ibm_db.close(ibm_db_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate Pruned Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "ibm_db_conn, conn = connect_to_schema(schema,conn_str)\n",
    "\n",
    "sql = \"\"\"CALL IDAX.CONFUSION_MATRIX('intable=T_TRAIN_CLEANED, id=PASSENGERID, target=SURVIVED, \n",
    "        resulttable=T_DT_P_TRAIN_PREDICTIONS, matrixtable=T_DT_P_TRAIN_CM');\"\"\"\n",
    "stmt = ibm_db.exec_immediate(ibm_db_conn, sql)\n",
    "\n",
    "sql = \"\"\"CALL IDAX.CONFUSION_MATRIX('intable=T_TEST_CLEANED, id=PASSENGERID, target=SURVIVED, \n",
    "        resulttable=T_DT_P_TEST_PREDICTIONS, matrixtable=T_DT_P_TEST_CM');\"\"\"\n",
    "stmt = ibm_db.exec_immediate(ibm_db_conn, sql)\n",
    "\n",
    "\n",
    "rc = ibm_db.close(ibm_db_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print confusion matrix\n",
    "print('Unpruned model on Training Set:')\n",
    "plot_conf_mtx('T_DT_P_TRAIN_CM')\n",
    "plt.show()\n",
    "print('Unpruned model on Test Set:')\n",
    "plot_conf_mtx('T_DT_P_TEST_CM')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix Statistics for pruned tree on training and test sets\n",
    "print('Unpruned Model on Training Set:')\n",
    "get_conf_mtx_stats('T_DT_P_TRAIN_CM')\n",
    "print('Unpruned Model on Test Set:')\n",
    "get_conf_mtx_stats('T_DT_P_TEST_CM')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "Pruned model performance:\n",
    "- Pruned Training Acc: 83.9% (-0.5%)\n",
    "- Pruned Test Acc: 78.4% (+2%)\n",
    "\n",
    "We can be confident that this pruned model will perform better on new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Deployed Model<a class=\"anchor\" id=\"Deployment\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on new data\n",
    "ibm_db_conn, conn = connect_to_schema(schema,conn_str)\n",
    "\n",
    "sql = \"\"\"CALL IDAX.PREDICT_DECTREE('model=titanic_dt, intable=T_VAL_CLEANED, \n",
    "        outtable=T_PREDICTIONS')\"\"\"\n",
    "stmt = ibm_db.exec_immediate(ibm_db_conn, sql)\n",
    "\n",
    "sql=\"select * FROM T_PREDICTIONS;\"\n",
    "df = pd.read_sql(sql,conn)\n",
    "\n",
    "rc = ibm_db.close(ibm_db_conn)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Conclusion<a class=\"anchor\" id=\"Conclusion\"></a>\n",
    "\n",
    "In this notebook we have demonstrated the use of SQL Stored Procedures in building an end-to-end machine learning pipeline (data exploration, data transformation, model building, model tuning, and model evaluation) that is executed entirely in-database.\n",
    "\n",
    "|Model | Training Accuracy | Testing Accuracy| Precision (Test, Class 1) | Recall (Test, Class 1)|\n",
    "| --- | --- |--- | --- | --- |\n",
    "|Unpruned DTC | 84.4% | 76.4%|71.6%|67.6%|\n",
    "|Pruned DTC | 83.9% | 78.4%|79.6% | 60.6%|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Cleaning up Model and Tables<a class=\"anchor\" id=\"Cleaning-up\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outstanding_tables(conn_str, intable_name, schema, print_error=False):\n",
    "    # Delete all outstanding Tables\n",
    "    ibm_db_conn = ibm_db.connect(conn_str,\"\",\"\")\n",
    "    conn = ibm_db_dbi.Connection(ibm_db_conn) \n",
    "    # drop summary\n",
    "    try:\n",
    "            sql = \"CALL IDAX.DROP_SUMMARY1000('intable= \" + schema + \".T_TRAIN_STATS')\"\n",
    "            stmt = ibm_db.exec_immediate(ibm_db_conn, sql)  \n",
    "    except:\n",
    "            if print_error: print(\"DROP_SUMMARY1000 removal error, skiping removal\")  \n",
    "    #______________________________________________________________________________________________\n",
    "    # drop view\n",
    "    try:\n",
    "            sql= \"DROP VIEW \" + schema + \".\" + intable_name + \"_VIEW\"\n",
    "            stmt = ibm_db.exec_immediate(ibm_db_conn, sql) \n",
    "    except:\n",
    "            if print_error: print(schema + \".\" + intable_name +\"view removal error, skiping removal\")\n",
    "    #______________________________________________________________________________________________\n",
    "    # drop trainded models\n",
    "    try:\n",
    "            sql= \"CALL IDAX.DROP_MODEL('model= \" + schema + \".titanic_dt')\"\n",
    "            stmt = ibm_db.exec_immediate(ibm_db_conn, sql)\n",
    "    except:\n",
    "            if print_error: print(\"DROP_MODEL error, skiping removal\")  \n",
    "    try:\n",
    "            sql= \"CALL IDAX.DROP_MODEL('model= \" + schema + \".titanic_knn')\"\n",
    "            stmt = ibm_db.exec_immediate(ibm_db_conn, sql)\n",
    "    except:\n",
    "            if print_error: print(\"DROP_MODEL error, skiping removal\")  \n",
    "    try:\n",
    "            sql= \"CALL IDAX.DROP_MODEL('model= \" + schema + \".titanic_nb')\"\n",
    "            stmt = ibm_db.exec_immediate(ibm_db_conn, sql)\n",
    "    except:\n",
    "            if print_error: print(\"DROP_MODEL error, skiping removal\")  \n",
    "    #______________________________________________________________________________________________\n",
    "    # drop any other table created in the schema\n",
    "    get_tables_sql = \"select NAME from sysibm.systables where CREATOR = '\" + schema +\"'\"\n",
    "    created_tables = pd.read_sql(get_tables_sql,conn).values.tolist()\n",
    "    for table_list in created_tables:\n",
    "        for table in table_list:\n",
    "            sql= \"DROP TABLE \" + schema + \".\" + table\n",
    "            try:\n",
    "                stmt = ibm_db.exec_immediate(ibm_db_conn, sql)\n",
    "            except:\n",
    "                if print_error: print(table +\" removal error, skiping removal\")\n",
    "    rc = ibm_db.close(ibm_db_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_outstanding_tables(conn_str, \"TITANIC\", \"CLASS\", print_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all tables properly deleted\n",
    "ibm_db_conn = ibm_db.connect(conn_str,\"\",\"\")\n",
    "conn = ibm_db_dbi.Connection(ibm_db_conn)\n",
    "sql=\"select NAME, CREATOR from sysibm.systables where CREATOR = 'CLASS';\"\n",
    "df = pd.read_sql(sql,conn)\n",
    "\n",
    "rc = ibm_db.close(ibm_db_conn)\n",
    "print('Connection Closed:',rc)\n",
    "\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
