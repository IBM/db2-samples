{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6fc14aaf-adf3-4344-8d21-a79b57911791"
   },
   "source": [
    "# Training a Machine Learning Model Externally and Deploying it to Db2 for In-Database Scoring\n",
    "\n",
    "In this notebook, we will train a customer segmentation model using the K-Means algorithm model externally in IBM Cloud Pak for Data. The model will then be deployed to Db2 as a joblib file. We will then use Db2's Python UDFs to execute the inferencing pipeline and return predictions to the user.\n",
    "\n",
    "Contents:\n",
    "<br>\n",
    "**I. Model Training** - Train a customer segmentation model in Watson Studio and create deployment assets (e.g. trained model, column metadata, PCA components, etc.)<br>\n",
    "**II. Model Deployment** - Using Python UDFs, deploy the inferencing pipeline (i.e. data processing and model scoring) to the database for in-database scoring<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "59f8d322-9e14-4a17-8821-4088ce6b147c"
   },
   "source": [
    "# I. Training a Customer Segmentation Model with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "872d6494-4aa5-418c-9d89-86ba11be3f6e"
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ff9fb5e-fe48-4b1b-ba87-e2035d19499f"
   },
   "source": [
    "In this section we will train our customer segmentation model. Segmentation can help you to identify and understand customer subgroups and how they differ from one another. Customer segmentation has many beneficial uses. In our case for example, it can be used to assign the best-fit agent, identify opportunities for leveraging agent skills, or even to create hosting opportunities to improve client engagement.<br><br>\n",
    "We'll prepare the data so it's in a wide format ready for segmentation and then apply some transformation and clustering techniques. Our data contains demographic, behavioural, summary product and communication based data. Since there are currently no predefined segments we'll need to discover segments within the underlying data structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0e064d99-7d12-4b05-a88b-cda816973b42"
   },
   "source": [
    "**Sample Materials, provided under license. <br>\n",
    "Licensed Materials - Property of IBM. <br>\n",
    "© Copyright IBM Corp. 2019, 2020. All Rights Reserved. <br>\n",
    "US Government Users Restricted Rights - Use, duplication or disclosure restricted by GSA ADP Schedule Contract with IBM Corp. <br>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0ea13466-bdf8-4c67-a191-86ad97074e41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn==0.23.2\n",
      "  Downloading scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.8 MB 8.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from scikit-learn==0.23.2) (2.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from scikit-learn==0.23.2) (1.5.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from scikit-learn==0.23.2) (0.16.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from scikit-learn==0.23.2) (1.18.5)\n",
      "Installing collected packages: scikit-learn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.23.1\n",
      "    Uninstalling scikit-learn-0.23.1:\n",
      "      Successfully uninstalled scikit-learn-0.23.1\n",
      "Successfully installed scikit-learn-0.23.2\n"
     ]
    }
   ],
   "source": [
    "# # Ensure sklearn is the correct version\n",
    "!pip install scikit-learn==0.23.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "877795d6-01dc-4f0a-bbfd-740c8b539c75"
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.api.types import is_string_dtype\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "import sys\n",
    "import joblib\n",
    "import seaborn as sns\n",
    "\n",
    "from project_lib import Project\n",
    "project = Project()\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "if '/project_data/data_asset/' not in sys.path:\n",
    "    sys.path.insert(0, '/project_data/data_asset/')\n",
    "    \n",
    "from customer_segmentation_prep import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "d939c005-8aea-4b1c-acf2-2bd6fccdb1d1"
   },
   "outputs": [],
   "source": [
    "#Establishing Connection to Database\n",
    "\n",
    "import pandas as pd\n",
    "import ibm_db_dbi\n",
    "import ibm_db\n",
    "from project_lib import Project \n",
    "\n",
    "# Define connection string and connect to database\n",
    "project = Project.access()\n",
    "LocalDB2_credentials = project.get_connection(name = \"CSSDB3\")\n",
    "\n",
    "bluedb_connection = ibm_db.connect(\"DATABASE={};HOSTNAME={};PORT={};PROTOCOL=TCPIP;UID={};PWD={}\".format(LocalDB2_credentials['database'],\n",
    "                                                                                                         LocalDB2_credentials['host'],\n",
    "                                                                                                         LocalDB2_credentials['port'],\n",
    "                                                                                                         LocalDB2_credentials['username'],\n",
    "                                                                                                         LocalDB2_credentials['password']),\"\",\"\")\n",
    "dbi_bluedb_connection = ibm_db_dbi.Connection(bluedb_connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8f416669-3296-4885-9c25-228f5df66929"
   },
   "source": [
    "## User Inputs and Data Prep\n",
    "\n",
    "### User Inputs\n",
    "**effective_date :**  This is the date that the segmentation is computed. All input data should be before this date.<br>\n",
    "**train_or_score :**  Specify whether we are prepping the data for training or scoring. Should always be 'train' in this notebook.<br>\n",
    "\n",
    "**granularity_key :** Specifies the customer ID column.<br>\n",
    "**customer_start_date :** Column with the start of the summary month of customer data.<br> \n",
    "**customer_end_date :** As above, but last day of the summary month.<br>\n",
    "**status_attribute :** Column which indicates whether the customer is active or inactive and is used to define churn. Churned customers are removed from the dataset.<br>\n",
    "**status_flag_active :** The name of the variable in the status_attribute that indicates that the customer has churned, in this case it is 'Inactive'.<br>\n",
    "**date_customer_joined :** Specifies the column where the customer join date is recorded. This variable is used to calculate customer tenure.<br>\n",
    "\n",
    "**columns_required :** A list of default columns required, includes ID column and date columns.<br>\n",
    "**default_attributes :** A list of the variables that we would like to use for the segmentation.<br>\n",
    "**risk_tolerance_list :** A list of the risk categories for the customer's accounts. 'High', 'Low' etc.<br> \n",
    "**investment_objective_list :** A list of the investment objective categories for the customer's accounts. 'Security', 'Income' etc.<br>\n",
    "\n",
    "The last three user input variables are used for data cleaning.<br>\n",
    "**std_multiplier :** This variable is used to identify outlier values. This number is multiplied by the variable standard deviation. Any value above this is defined as an outlier and the value is capped at this number multiplied by the standard deviation.<br>\n",
    "**max_num_cat_cardinality :** This variable defines the maximum cardinality for categorical variables. Any categorical variable with more categories than this maximum is removed from the dataset.<br> \n",
    "**nulls_threshold :** This threshold is used to identify columns with many null values. Any column with percentage of nulls greater than this threshold will be removed from the dataset.<br>\n",
    "\n",
    "The user can use the default inputs as listed below or can choose their own. The user inputs will be stored and the same inputs will be applied automatically at scoring time. \n",
    "\n",
    "\n",
    "### Data Prep\n",
    "See `project_data/data_asset/customer_segmentation_prep.py` for details of data preparation.\n",
    "\n",
    "The script generates the dataset that is used for clustering. We take a wide form dataset with customer details, filter to include only columns that are relevant, complete data cleaning and produce a dataframe suitable for clustering. \n",
    "\n",
    "### Data Cleaning\n",
    "•\tAny customer who attrited in the dataset is removed. Only active customers are used for clustering.<br>\n",
    "•\tWe take the most recent record for each customer.<br>\n",
    "•\tAny columns in the dataset that have a single constant value are removed.<br>\n",
    "•\tAny column with more than 10% null values is removed.<br>\n",
    "•\tHigh cardinality categorical columns are removed.<br>\n",
    "•\tNumerical outliers are cleaned. <br>\n",
    "•\tRemaining missing values are filled with 'Unknown' for categorical and the average of the column for numerical. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "a0cf16f7-8cbb-4b4b-bf60-d00c70629140"
   },
   "outputs": [],
   "source": [
    "# User input variables\n",
    "effective_date = '2018-09-30'  # date at which the prediction was computed \n",
    "train_or_score = 'train'\n",
    "\n",
    "granularity_key='CUSTOMER_CUSTOMER_ID'\n",
    "customer_start_date='CUSTOMER_SUMMARY_START_DATE'\n",
    "customer_end_date='CUSTOMER_SUMMARY_END_DATE'\n",
    "status_attribute='CUSTOMER_STATUS'\n",
    "status_flag_active='Active'\n",
    "date_customer_joined='CUSTOMER_RELATIONSHIP_START_DATE'\n",
    "\n",
    "columns_required=['CUSTOMER_CUSTOMER_ID', 'CUSTOMER_STATUS', 'CUSTOMER_SUMMARY_START_DATE', 'CUSTOMER_SUMMARY_END_DATE',\n",
    "                    'CUSTOMER_EFFECTIVE_DATE',  'CUSTOMER_SYSTEM_LOAD_TIMESTAMP']\n",
    "\n",
    "default_attributes=['CUSTOMER_GENDER', 'CUSTOMER_AGE_RANGE', 'CUSTOMER_EDUCATION_LEVEL',\n",
    "                            'CUSTOMER_EMPLOYMENT_STATUS', 'CUSTOMER_MARITAL_STATUS', \n",
    "                            'CUSTOMER_URBAN_CODE', 'CUSTOMER_ANNUAL_INCOME', 'CUSTOMER_RELATIONSHIP_START_DATE', \n",
    "                            'CUSTOMER_SUMMARY_RETURN_LAST_QUARTER', \n",
    "                            'CUSTOMER_SUMMARY_NUMBER_OF_EMAILS',\n",
    "                            'CUSTOMER_SUMMARY_NUMBER_OF_LOGINS',\n",
    "                    'CUSTOMER_SUMMARY_AMOUNT_OF_MANAGEMENT_FEES',\n",
    "                           'CUSTOMER_SUMMARY_TOP_SPENDING_CATEGORY', 'CUSTOMER_CREDIT_AUTHORITY_LEVEL', 'CUSTOMER_CUSTOMER_BEHAVIOR', 'CUSTOMER_IMPORTANCE_LEVEL_CODE',\n",
    "                           'CUSTOMER_MARKET_GROUP',\n",
    "                           'CUSTOMER_PURSUIT']\n",
    "risk_tolerance_list = []\n",
    "investment_objective_list = []\n",
    "\n",
    "std_multiplier=5\n",
    "max_num_cat_cardinality=15\n",
    "nulls_threshold=0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a039a37d-d642-4595-9def-c8c69cdcbf94"
   },
   "source": [
    "## Load Customer Segmentation Data\n",
    "\n",
    "For this project we will be loading our training data from the database table called **DSE.CUST_SEG_DATA_TRAIN**. This table was created from the csv file **customer_full_summary_latest.csv**. The file is located in the `/project_data/data_asset/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "cb69f873-c152-4557-b71d-e2e60845e1da",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removing inactive customers we have 1000 customers\n",
      "After removing inactive customers we have 838 customers\n",
      "Before cleaning, we had 22 columns.\n",
      "After cleaning, we have 19 columns.\n",
      "Add a column for customer tenure\n",
      "Prepped data has 838 rows and 17 columns.\n",
      "Prep has data for 838 customers\n"
     ]
    }
   ],
   "source": [
    "# Read in training data from Db2\n",
    "sql = 'SELECT * FROM DSE.CUST_SEG_DATA_TRAIN'\n",
    "input_df = pd.read_sql(sql, dbi_bluedb_connection)\n",
    "\n",
    "# Convert Db2 datatypes to Python datatypes - important for datetime columns!\n",
    "original_csv_dtypes = joblib.load('/project_data/data_asset/original_csv_dtypes.pkl') \n",
    "df_raw = input_df.astype(original_csv_dtypes)\n",
    "\n",
    "data_prep = CustomerSegmentationPrep(train_or_score=train_or_score, effective_date=effective_date, granularity_key=granularity_key, customer_start_date=customer_start_date, customer_end_date=customer_end_date,\n",
    "                                        status_attribute=status_attribute, status_flag_active=status_flag_active, date_customer_joined=date_customer_joined, columns_required=columns_required, default_attributes=default_attributes,\n",
    "                                        risk_tolerance_list=risk_tolerance_list, investment_objective_list=investment_objective_list, std_multiplier=std_multiplier, max_num_cat_cardinality=max_num_cat_cardinality, nulls_threshold=nulls_threshold)\n",
    "df_prepped = data_prep.prep_data(df_raw, train_or_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5a0fa204-0fd5-4ea2-bd23-f73d5c754164",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CUSTOMER_CUSTOMER_ID</th>\n",
       "      <th>CUSTOMER_PURSUIT</th>\n",
       "      <th>CUSTOMER_CREDIT_AUTHORITY_LEVEL</th>\n",
       "      <th>CUSTOMER_EMPLOYMENT_STATUS</th>\n",
       "      <th>CUSTOMER_MARKET_GROUP</th>\n",
       "      <th>CUSTOMER_AGE_RANGE</th>\n",
       "      <th>CUSTOMER_EDUCATION_LEVEL</th>\n",
       "      <th>CUSTOMER_MARITAL_STATUS</th>\n",
       "      <th>CUSTOMER_CUSTOMER_BEHAVIOR</th>\n",
       "      <th>CUSTOMER_SUMMARY_AMOUNT_OF_MANAGEMENT_FEES</th>\n",
       "      <th>CUSTOMER_SUMMARY_TOP_SPENDING_CATEGORY</th>\n",
       "      <th>CUSTOMER_URBAN_CODE</th>\n",
       "      <th>CUSTOMER_EFFECTIVE_DATE</th>\n",
       "      <th>CUSTOMER_GENDER</th>\n",
       "      <th>CUSTOMER_ANNUAL_INCOME</th>\n",
       "      <th>CUSTOMER_IMPORTANCE_LEVEL_CODE</th>\n",
       "      <th>CUSTOMER_TENURE_IN_MONTHS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000</td>\n",
       "      <td>Capital Acquisition</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Employed</td>\n",
       "      <td>Accumulating</td>\n",
       "      <td>30 to 40</td>\n",
       "      <td>College</td>\n",
       "      <td>Married</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>1757.13</td>\n",
       "      <td>Recreation</td>\n",
       "      <td>City</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>Male</td>\n",
       "      <td>325000.0</td>\n",
       "      <td>Low priority</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001</td>\n",
       "      <td>Retirement Planning</td>\n",
       "      <td>Very High</td>\n",
       "      <td>Selfemployed</td>\n",
       "      <td>Gifting</td>\n",
       "      <td>65 and over</td>\n",
       "      <td>Professional</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Aggressive</td>\n",
       "      <td>17935.79</td>\n",
       "      <td>Uncategorized</td>\n",
       "      <td>Urban</td>\n",
       "      <td>2017-11-29</td>\n",
       "      <td>Female</td>\n",
       "      <td>280000.0</td>\n",
       "      <td>Normal priority</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1002</td>\n",
       "      <td>Increase Net Worth</td>\n",
       "      <td>Very Low</td>\n",
       "      <td>Homemaker</td>\n",
       "      <td>Accumulating</td>\n",
       "      <td>55 to 65</td>\n",
       "      <td>PhD</td>\n",
       "      <td>Married</td>\n",
       "      <td>Growth</td>\n",
       "      <td>1221.06</td>\n",
       "      <td>Travel</td>\n",
       "      <td>Urban</td>\n",
       "      <td>2017-08-28</td>\n",
       "      <td>Female</td>\n",
       "      <td>130000.0</td>\n",
       "      <td>High priority</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1003</td>\n",
       "      <td>Increase Net Worth</td>\n",
       "      <td>Very Low</td>\n",
       "      <td>Homemaker</td>\n",
       "      <td>Accumulating</td>\n",
       "      <td>65 and over</td>\n",
       "      <td>PhD</td>\n",
       "      <td>Married</td>\n",
       "      <td>Growth</td>\n",
       "      <td>1176.59</td>\n",
       "      <td>Travel</td>\n",
       "      <td>Urban</td>\n",
       "      <td>2018-01-17</td>\n",
       "      <td>Female</td>\n",
       "      <td>120000.0</td>\n",
       "      <td>High priority</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1004</td>\n",
       "      <td>Estate Planning</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Employed</td>\n",
       "      <td>Accumulating</td>\n",
       "      <td>40 to 55</td>\n",
       "      <td>College</td>\n",
       "      <td>Married</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>14452.36</td>\n",
       "      <td>Food</td>\n",
       "      <td>City</td>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>Male</td>\n",
       "      <td>350000.0</td>\n",
       "      <td>Low priority</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CUSTOMER_CUSTOMER_ID     CUSTOMER_PURSUIT CUSTOMER_CREDIT_AUTHORITY_LEVEL  \\\n",
       "0                  1000  Capital Acquisition                          Medium   \n",
       "1                  1001  Retirement Planning                       Very High   \n",
       "2                  1002   Increase Net Worth                        Very Low   \n",
       "3                  1003   Increase Net Worth                        Very Low   \n",
       "4                  1004      Estate Planning                          Medium   \n",
       "\n",
       "  CUSTOMER_EMPLOYMENT_STATUS CUSTOMER_MARKET_GROUP CUSTOMER_AGE_RANGE  \\\n",
       "0                   Employed          Accumulating           30 to 40   \n",
       "1               Selfemployed               Gifting        65 and over   \n",
       "2                  Homemaker          Accumulating           55 to 65   \n",
       "3                  Homemaker          Accumulating        65 and over   \n",
       "4                   Employed          Accumulating           40 to 55   \n",
       "\n",
       "  CUSTOMER_EDUCATION_LEVEL CUSTOMER_MARITAL_STATUS CUSTOMER_CUSTOMER_BEHAVIOR  \\\n",
       "0                  College                 Married                   Moderate   \n",
       "1             Professional                Divorced                 Aggressive   \n",
       "2                      PhD                 Married                     Growth   \n",
       "3                      PhD                 Married                     Growth   \n",
       "4                  College                 Married                   Moderate   \n",
       "\n",
       "   CUSTOMER_SUMMARY_AMOUNT_OF_MANAGEMENT_FEES  \\\n",
       "0                                     1757.13   \n",
       "1                                    17935.79   \n",
       "2                                     1221.06   \n",
       "3                                     1176.59   \n",
       "4                                    14452.36   \n",
       "\n",
       "  CUSTOMER_SUMMARY_TOP_SPENDING_CATEGORY CUSTOMER_URBAN_CODE  \\\n",
       "0                             Recreation                City   \n",
       "1                          Uncategorized               Urban   \n",
       "2                                 Travel               Urban   \n",
       "3                                 Travel               Urban   \n",
       "4                                   Food                City   \n",
       "\n",
       "  CUSTOMER_EFFECTIVE_DATE CUSTOMER_GENDER  CUSTOMER_ANNUAL_INCOME  \\\n",
       "0              2018-01-02            Male                325000.0   \n",
       "1              2017-11-29          Female                280000.0   \n",
       "2              2017-08-28          Female                130000.0   \n",
       "3              2018-01-17          Female                120000.0   \n",
       "4              2018-01-03            Male                350000.0   \n",
       "\n",
       "  CUSTOMER_IMPORTANCE_LEVEL_CODE  CUSTOMER_TENURE_IN_MONTHS  \n",
       "0                   Low priority                          8  \n",
       "1                Normal priority                         10  \n",
       "2                  High priority                         13  \n",
       "3                  High priority                          8  \n",
       "4                   Low priority                          8  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview prepped data\n",
    "df_prepped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4b178763-1220-407a-899c-9fec6c47552b"
   },
   "source": [
    "Now that the data is prepared we need to continue with a few more data preparation steps before we can do clustering. First is to simply remove the columns `CUSTOMER_CUSTOMER_ID` and `CUSTOMER_EFFECTIVE_DATE` since they're not needed for segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "3684de3b-7f2d-431e-b55c-41881b3055df"
   },
   "outputs": [],
   "source": [
    "# Drop columns not needed for segmentation\n",
    "df_prepped.drop(['CUSTOMER_CUSTOMER_ID', 'CUSTOMER_EFFECTIVE_DATE'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0917c757-2af1-4a95-841b-e97f7af2ebeb"
   },
   "source": [
    "### Dummy Variables\n",
    "\n",
    "Next, since our data contains mixed data types, categorical and numeric, we need to convert those categorical features to numeric by creating binary dummy variables. Once we create the dummy variables from the categorical features we'll drop the original categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "61c0dc65-27bb-46e2-b0ea-82cc2f6b805f"
   },
   "outputs": [],
   "source": [
    "# Create lists of the numeric and categorical features\n",
    "numeric_cols = list(df_prepped.select_dtypes(include=[np.number]).columns)\n",
    "categorical_cols = list(df_prepped.select_dtypes(include=[object]).columns)\n",
    "\n",
    "# Copy of the prepped dataframe before any transformations are carried out\n",
    "prepped_data_pre_transform = df_prepped.copy()\n",
    "\n",
    "# Create dummy variables for categorical features and drop original\n",
    "for col in categorical_cols:\n",
    "    df_prepped = pd.concat([df_prepped, pd.get_dummies(df_prepped[col], prefix=col, drop_first=True)], axis=1)\n",
    "    df_prepped.drop(col, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "31fd912f-8199-4e66-8833-3f2319089c4f"
   },
   "source": [
    "### Standardize Data\n",
    "\n",
    "The last step for our data preparation is to standardize the numeric variables. Standardizing numeric values prior to clustering is common practice especially when dealing with features of varying scales (number of children vs summary of assets amount). This helps to improve the cluster quality as well as cluster algorithm accuracy and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "839f78cd-11ae-4bd8-974c-6e73720ec3a1"
   },
   "outputs": [],
   "source": [
    "scale_features = df_prepped[numeric_cols]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scale_features = scaler.fit_transform(scale_features.values)\n",
    "df_prepped[numeric_cols] = scale_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "df9fd265-eea4-4238-bc94-d3b773b66980"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CUSTOMER_SUMMARY_AMOUNT_OF_MANAGEMENT_FEES</th>\n",
       "      <th>CUSTOMER_ANNUAL_INCOME</th>\n",
       "      <th>CUSTOMER_TENURE_IN_MONTHS</th>\n",
       "      <th>CUSTOMER_PURSUIT_Education Planning</th>\n",
       "      <th>CUSTOMER_PURSUIT_Estate Planning</th>\n",
       "      <th>CUSTOMER_PURSUIT_Increase Net Worth</th>\n",
       "      <th>CUSTOMER_PURSUIT_Philanthropy</th>\n",
       "      <th>CUSTOMER_PURSUIT_Retirement Planning</th>\n",
       "      <th>CUSTOMER_CREDIT_AUTHORITY_LEVEL_Low</th>\n",
       "      <th>CUSTOMER_CREDIT_AUTHORITY_LEVEL_Medium</th>\n",
       "      <th>...</th>\n",
       "      <th>CUSTOMER_SUMMARY_TOP_SPENDING_CATEGORY_Recreation</th>\n",
       "      <th>CUSTOMER_SUMMARY_TOP_SPENDING_CATEGORY_Savings</th>\n",
       "      <th>CUSTOMER_SUMMARY_TOP_SPENDING_CATEGORY_Travel</th>\n",
       "      <th>CUSTOMER_SUMMARY_TOP_SPENDING_CATEGORY_Uncategorized</th>\n",
       "      <th>CUSTOMER_SUMMARY_TOP_SPENDING_CATEGORY_Utilities</th>\n",
       "      <th>CUSTOMER_URBAN_CODE_Rural</th>\n",
       "      <th>CUSTOMER_URBAN_CODE_Urban</th>\n",
       "      <th>CUSTOMER_GENDER_Male</th>\n",
       "      <th>CUSTOMER_IMPORTANCE_LEVEL_CODE_Low priority</th>\n",
       "      <th>CUSTOMER_IMPORTANCE_LEVEL_CODE_Normal priority</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.850833</td>\n",
       "      <td>0.932255</td>\n",
       "      <td>-0.897529</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.167705</td>\n",
       "      <td>0.573600</td>\n",
       "      <td>-0.701125</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.950851</td>\n",
       "      <td>-0.621916</td>\n",
       "      <td>-0.406519</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.959148</td>\n",
       "      <td>-0.701617</td>\n",
       "      <td>-0.897529</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.517783</td>\n",
       "      <td>1.131508</td>\n",
       "      <td>-0.897529</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   CUSTOMER_SUMMARY_AMOUNT_OF_MANAGEMENT_FEES  CUSTOMER_ANNUAL_INCOME  \\\n",
       "0                                   -0.850833                0.932255   \n",
       "1                                    2.167705                0.573600   \n",
       "2                                   -0.950851               -0.621916   \n",
       "3                                   -0.959148               -0.701617   \n",
       "4                                    1.517783                1.131508   \n",
       "\n",
       "   CUSTOMER_TENURE_IN_MONTHS  CUSTOMER_PURSUIT_Education Planning  \\\n",
       "0                  -0.897529                                    0   \n",
       "1                  -0.701125                                    0   \n",
       "2                  -0.406519                                    0   \n",
       "3                  -0.897529                                    0   \n",
       "4                  -0.897529                                    0   \n",
       "\n",
       "   CUSTOMER_PURSUIT_Estate Planning  CUSTOMER_PURSUIT_Increase Net Worth  \\\n",
       "0                                 0                                    0   \n",
       "1                                 0                                    0   \n",
       "2                                 0                                    1   \n",
       "3                                 0                                    1   \n",
       "4                                 1                                    0   \n",
       "\n",
       "   CUSTOMER_PURSUIT_Philanthropy  CUSTOMER_PURSUIT_Retirement Planning  \\\n",
       "0                              0                                     0   \n",
       "1                              0                                     1   \n",
       "2                              0                                     0   \n",
       "3                              0                                     0   \n",
       "4                              0                                     0   \n",
       "\n",
       "   CUSTOMER_CREDIT_AUTHORITY_LEVEL_Low  \\\n",
       "0                                    0   \n",
       "1                                    0   \n",
       "2                                    0   \n",
       "3                                    0   \n",
       "4                                    0   \n",
       "\n",
       "   CUSTOMER_CREDIT_AUTHORITY_LEVEL_Medium  ...  \\\n",
       "0                                       1  ...   \n",
       "1                                       0  ...   \n",
       "2                                       0  ...   \n",
       "3                                       0  ...   \n",
       "4                                       1  ...   \n",
       "\n",
       "   CUSTOMER_SUMMARY_TOP_SPENDING_CATEGORY_Recreation  \\\n",
       "0                                                  1   \n",
       "1                                                  0   \n",
       "2                                                  0   \n",
       "3                                                  0   \n",
       "4                                                  0   \n",
       "\n",
       "   CUSTOMER_SUMMARY_TOP_SPENDING_CATEGORY_Savings  \\\n",
       "0                                               0   \n",
       "1                                               0   \n",
       "2                                               0   \n",
       "3                                               0   \n",
       "4                                               0   \n",
       "\n",
       "   CUSTOMER_SUMMARY_TOP_SPENDING_CATEGORY_Travel  \\\n",
       "0                                              0   \n",
       "1                                              0   \n",
       "2                                              1   \n",
       "3                                              1   \n",
       "4                                              0   \n",
       "\n",
       "   CUSTOMER_SUMMARY_TOP_SPENDING_CATEGORY_Uncategorized  \\\n",
       "0                                                  0      \n",
       "1                                                  1      \n",
       "2                                                  0      \n",
       "3                                                  0      \n",
       "4                                                  0      \n",
       "\n",
       "   CUSTOMER_SUMMARY_TOP_SPENDING_CATEGORY_Utilities  \\\n",
       "0                                                 0   \n",
       "1                                                 0   \n",
       "2                                                 0   \n",
       "3                                                 0   \n",
       "4                                                 0   \n",
       "\n",
       "   CUSTOMER_URBAN_CODE_Rural  CUSTOMER_URBAN_CODE_Urban  CUSTOMER_GENDER_Male  \\\n",
       "0                          0                          0                     1   \n",
       "1                          0                          1                     0   \n",
       "2                          0                          1                     0   \n",
       "3                          0                          1                     0   \n",
       "4                          0                          0                     1   \n",
       "\n",
       "   CUSTOMER_IMPORTANCE_LEVEL_CODE_Low priority  \\\n",
       "0                                            1   \n",
       "1                                            0   \n",
       "2                                            0   \n",
       "3                                            0   \n",
       "4                                            1   \n",
       "\n",
       "   CUSTOMER_IMPORTANCE_LEVEL_CODE_Normal priority  \n",
       "0                                               0  \n",
       "1                                               1  \n",
       "2                                               0  \n",
       "3                                               0  \n",
       "4                                               0  \n",
       "\n",
       "[5 rows x 47 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview prepped data with standardized numeric values\n",
    "df_prepped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3593f646-d580-4cf2-aef2-3b3514515e5a"
   },
   "source": [
    "## Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "109948f5-d8d8-46ae-ba6d-99e43518ce06"
   },
   "source": [
    "Now that our data is clean with some transformations we're going to do one last transformation. We'll use principal components analysis (PCA) to reduce the dimensionality of our data and reduce clustering computation. Also, performing PCA prior to clustering can often help with performance if there are underlying linear relationships with the data. This isn't always the case and is dependent on your data, but it did hold true with our customer dataset.\n",
    "\n",
    "Below we perform PCA on our dataset and select 14 components. You can see the newly transformed and reduced data set below. This will be the data set that we pass to our clustering algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "c49b36dc-1987-42f7-b2e9-bcb51597fa82"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.633069</td>\n",
       "      <td>0.313649</td>\n",
       "      <td>-0.425015</td>\n",
       "      <td>0.899856</td>\n",
       "      <td>-0.658682</td>\n",
       "      <td>1.346514</td>\n",
       "      <td>0.145198</td>\n",
       "      <td>0.203640</td>\n",
       "      <td>-0.129012</td>\n",
       "      <td>-0.038811</td>\n",
       "      <td>0.131175</td>\n",
       "      <td>0.132386</td>\n",
       "      <td>-0.084905</td>\n",
       "      <td>-0.270533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.412770</td>\n",
       "      <td>1.449158</td>\n",
       "      <td>-1.563411</td>\n",
       "      <td>0.826072</td>\n",
       "      <td>-0.414704</td>\n",
       "      <td>-0.496564</td>\n",
       "      <td>0.013733</td>\n",
       "      <td>0.963539</td>\n",
       "      <td>-0.196875</td>\n",
       "      <td>0.225560</td>\n",
       "      <td>-0.688968</td>\n",
       "      <td>-0.154542</td>\n",
       "      <td>0.177696</td>\n",
       "      <td>0.054096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.275778</td>\n",
       "      <td>-0.993911</td>\n",
       "      <td>1.886191</td>\n",
       "      <td>0.385648</td>\n",
       "      <td>0.002758</td>\n",
       "      <td>0.249282</td>\n",
       "      <td>0.753729</td>\n",
       "      <td>0.430153</td>\n",
       "      <td>0.490053</td>\n",
       "      <td>0.696810</td>\n",
       "      <td>-0.194723</td>\n",
       "      <td>-0.643173</td>\n",
       "      <td>-0.127439</td>\n",
       "      <td>-0.249291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.530589</td>\n",
       "      <td>-1.085195</td>\n",
       "      <td>1.672390</td>\n",
       "      <td>0.855310</td>\n",
       "      <td>-0.271357</td>\n",
       "      <td>0.266506</td>\n",
       "      <td>0.281015</td>\n",
       "      <td>-0.008495</td>\n",
       "      <td>0.418908</td>\n",
       "      <td>0.740401</td>\n",
       "      <td>-0.531247</td>\n",
       "      <td>-0.803264</td>\n",
       "      <td>-0.146244</td>\n",
       "      <td>-0.657398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.680556</td>\n",
       "      <td>1.766475</td>\n",
       "      <td>-1.074686</td>\n",
       "      <td>1.096721</td>\n",
       "      <td>1.025809</td>\n",
       "      <td>0.680437</td>\n",
       "      <td>-0.147261</td>\n",
       "      <td>-0.195352</td>\n",
       "      <td>0.256169</td>\n",
       "      <td>0.172991</td>\n",
       "      <td>-0.220285</td>\n",
       "      <td>0.446379</td>\n",
       "      <td>0.019043</td>\n",
       "      <td>0.361261</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0 -1.633069  0.313649 -0.425015  0.899856 -0.658682  1.346514  0.145198   \n",
       "1  2.412770  1.449158 -1.563411  0.826072 -0.414704 -0.496564  0.013733   \n",
       "2  0.275778 -0.993911  1.886191  0.385648  0.002758  0.249282  0.753729   \n",
       "3  0.530589 -1.085195  1.672390  0.855310 -0.271357  0.266506  0.281015   \n",
       "4 -0.680556  1.766475 -1.074686  1.096721  1.025809  0.680437 -0.147261   \n",
       "\n",
       "          7         8         9        10        11        12        13  \n",
       "0  0.203640 -0.129012 -0.038811  0.131175  0.132386 -0.084905 -0.270533  \n",
       "1  0.963539 -0.196875  0.225560 -0.688968 -0.154542  0.177696  0.054096  \n",
       "2  0.430153  0.490053  0.696810 -0.194723 -0.643173 -0.127439 -0.249291  \n",
       "3 -0.008495  0.418908  0.740401 -0.531247 -0.803264 -0.146244 -0.657398  \n",
       "4 -0.195352  0.256169  0.172991 -0.220285  0.446379  0.019043  0.361261  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PCA\n",
    "pca = PCA(n_components=14)\n",
    "pca_fitted = pca.fit(df_prepped)\n",
    "df_pca = pca_fitted.transform(df_prepped)\n",
    "\n",
    "# Preview transformed data\n",
    "pd.DataFrame(df_pca).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bd080ded-c7cf-4da8-866a-04e1361cd29e"
   },
   "source": [
    "From our PCA, we can view how much variance is explained for each additional principal component. The below cumulative sum shows that the first 14 components explains 90.6% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "d043f0f6-c738-4210-bbc7-e3c9b239fb73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.21764406 0.38447951 0.52871062 0.64044143 0.70645547 0.74986159\n",
      " 0.78734266 0.81547564 0.83739013 0.85529122 0.87230691 0.88615574\n",
      " 0.89701825 0.90684308]\n"
     ]
    }
   ],
   "source": [
    "# Percent variance explained array\n",
    "print(np.cumsum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "019d899f-acc4-4065-966b-1c9f04c87eff"
   },
   "source": [
    "## K-means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28d016e6-069a-40ff-883b-0d641d209e0c"
   },
   "source": [
    "We can now begin clustering. For clustering we'll use the k-means clustering algorithm. K-means clustering simply partitions the data into k clusters where each observation or client belongs to the cluster with the nearest mean. When using k-means you have to specify the number of clusters beforehand, but often times that number is unknown. We'll loop through the k-means algorithm using a range of number of clusters and determine cluster number by using the metric called silhouette coefficient. The silhouette coefficient indicates how similar the observations within its own cluster are compared to other observations in different clusters. Range for silhouette coefficient are from -1 to 1 where 1 represents objects within its own cluster are well paired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "6faa5f92-6e78-4f09-901c-9f1377458aab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette coefficient for 2 clusters:0.22429690157253118\n",
      "Silhouette coefficient for 3 clusters:0.23397827199300802\n",
      "Silhouette coefficient for 4 clusters:0.27508986404202296\n",
      "Silhouette coefficient for 5 clusters:0.28634452973423685\n",
      "Silhouette coefficient for 6 clusters:0.30174119156635326\n",
      "Silhouette coefficient for 7 clusters:0.2941987400417092\n",
      "Silhouette coefficient for 8 clusters:0.2836505349160446\n",
      "Silhouette coefficient for 9 clusters:0.2666771048718334\n",
      "Silhouette coefficient for 10 clusters:0.2637364333328408\n",
      "Silhouette coefficient for 11 clusters:0.261700168501964\n",
      "Silhouette coefficient for 12 clusters:0.25686977767653274\n",
      "Silhouette coefficient for 13 clusters:0.25829634023463766\n",
      "Silhouette coefficient for 14 clusters:0.25242050363080915\n",
      "Silhouette coefficient for 15 clusters:0.2528373458169747\n"
     ]
    }
   ],
   "source": [
    "# Specify max number of clusters for iteration\n",
    "max_number_of_clusters = 15\n",
    "\n",
    "# Loop through K-means and view silhouette coefficient to determine number of clusters\n",
    "for i in range(2, max_number_of_clusters+1):\n",
    "    kmeans_mdl = KMeans(n_clusters=i, random_state=1234)\n",
    "    kmeans_mdl.fit(df_pca)\n",
    "    labels = kmeans_mdl.labels_\n",
    "    silhouette_coef = metrics.silhouette_score(df_pca, labels, metric='euclidean')\n",
    "    print('Silhouette coefficient for ' + str(i) + ' clusters:' + str(silhouette_coef))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8b77b00-b207-404c-b682-54f7038ff73e"
   },
   "source": [
    "For the above exercise we see that number of clusters should be 6 or 7 based on the silhouette coefficient. For this exercise we selected 7 clusters.\n",
    "\n",
    "We then fit the k-means algorithm to our data using the specified 7 clusters. Then we add those cluster assignments back to the PCA dataframe so we can visualize the cluster assignments on a 2-dimensional plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "bf4552c6-cb0a-4bff-a3b4-18bd018ef98b"
   },
   "outputs": [],
   "source": [
    "# K-means with 7 clusters based on silhouette coefficient\n",
    "num_clusters = 7\n",
    "kmeans_mdl = KMeans(n_clusters=num_clusters, random_state=1234)\n",
    "fitted_model = kmeans_mdl.fit(df_pca)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5a2e2d06e17c48b39c88576f17efc72d"
   },
   "source": [
    "# Save Deployment Assets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2ee8a6d55774df6b29f49ac119e6b25"
   },
   "source": [
    "### Save Data Transformation Assets to Shared Filesystem\n",
    "We finally save the deployment assets to a filesystem shared by the Db2 container and our Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "d72b6ca2f76d451080e12d422751f69b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/db2whjoblib/pca.txt']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# store the columns to be standardised, means and standard deviations in the training metadata \n",
    "with open('/db2whjoblib/training_data_metadata.json', 'r') as f:\n",
    "    training_metadata = json.load(f)\n",
    "\n",
    "training_metadata['cols_to_standardise'] = numeric_cols\n",
    "training_metadata['scaler_means'] = list(scaler.mean_)\n",
    "training_metadata['scaler_standard_dev'] = list(np.sqrt(scaler.var_))\n",
    "\n",
    "# store the names of the columns used for training\n",
    "training_metadata['cols_used_for_training'] = list(df_prepped.columns)\n",
    "\n",
    "with open('/db2whjoblib/training_data_metadata.json', 'w') as f:\n",
    "    json.dump(training_metadata, f)\n",
    "    \n",
    "# Save out PCA\n",
    "joblib.dump(pca_fitted, '/db2whjoblib/pca.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0b03996d-ebfd-404e-a854-e5227f1045fe"
   },
   "source": [
    "### Storing Kmeans Model as joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "42ceb86f-3d85-424a-8773-d9d28e226864"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/db2whjoblib/kmeans_mdl.joblib']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump\n",
    "\n",
    "#save model to db2 mounted directory\n",
    "dump(fitted_model,'/db2whjoblib/kmeans_mdl.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2977351e-0043-436f-b28e-b1281a869ac4"
   },
   "source": [
    "# II. Deploying the Trained Model and Inferencing Pipeline to Db2 for In-Database Scoring\n",
    "\n",
    "In this section, we will deploy our trained model to Db2 and use it to make inferences in-database.\n",
    "\n",
    "This section contains the following steps:\n",
    "1. Connect to Db2 Database - Connect to Db2\n",
    "2. Initalization of Environment - Installing Python packages on the Db2 platform and uploading deployment assets to the file system\n",
    "3. Writing the UDF - Write the UDF that will process and score the data\n",
    "4. UDF Function Deployment - Deploy the UDF function\n",
    "5. Make Predictions - Call the UDF and return the query results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "27fb7cb1-a3c9-49ee-b607-3867617dd6c9"
   },
   "source": [
    "## 1. Connect to Db2 Database\n",
    "\n",
    "Make sure you have a DB2 Warehouse instance and create a connection asset named \"CSSDB3\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "f3869603-23b6-478c-8362-4d350ebded57"
   },
   "outputs": [],
   "source": [
    "#Establishing Connection to Database\n",
    "\n",
    "import pandas as pd\n",
    "import ibm_db_dbi\n",
    "import ibm_db\n",
    "from project_lib import Project \n",
    "\n",
    "# Define connection string and connect to database\n",
    "project = Project.access()\n",
    "LocalDB2_credentials = project.get_connection(name = \"CSSDB3\")\n",
    "\n",
    "bluedb_connection = ibm_db.connect(\"DATABASE={};HOSTNAME={};PORT={};PROTOCOL=TCPIP;UID={};PWD={}\".format(LocalDB2_credentials['database'],\n",
    "                                                                                                         LocalDB2_credentials['host'],\n",
    "                                                                                                         LocalDB2_credentials['port'],\n",
    "                                                                                                         LocalDB2_credentials['username'],\n",
    "                                                                                                         LocalDB2_credentials['password']),\"\",\"\")\n",
    "dbi_bluedb_connection = ibm_db_dbi.Connection(bluedb_connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81a895ef-5662-4ebb-b586-69fb63d3e812"
   },
   "source": [
    "## 2. Initialization of Environment\n",
    "\n",
    "\n",
    "### Placing the necessary files and Python packages in the shared filesystem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0bdd1b6a-59f6-428a-8cf2-83e041c223fa"
   },
   "source": [
    "**Note:** Only run cells in section 2 if running the notebook for the first time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "a49a03d7-e49d-4765-94a9-8ff3952d86d1"
   },
   "outputs": [],
   "source": [
    "# # Only run this cell if running the notebook for the first time!\n",
    "\n",
    "# # copy original csv datatypes to pickle file to db2mount directory \n",
    "# !cp /project_data/data_asset/original_csv_dtypes.pkl /db2whjoblib/original_csv_dtypes.pkl\n",
    "\n",
    "\n",
    "# # copy data preparation file to db2mount directory \n",
    "# !cp /project_data/data_asset/customer_segmentation_prep.py /db2whjoblib/customer_segmentation_prep.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "e195e711-380d-4afe-8cd8-906acfbb2663"
   },
   "outputs": [],
   "source": [
    "# Only run this cell if running the notebook for the first time!\n",
    "\n",
    "# !pip install --target=/db2whjoblib/pandas pandas\n",
    "\n",
    "# !pip install --target=/db2whjoblib/dateutil python-dateutil\n",
    "\n",
    "# !pip install --target=/db2whjoblib/scikit-learn scikit-learn==0.23.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b9498244-a29a-4a57-a308-9f85b803e79c"
   },
   "source": [
    "## 3. Writing the UDF\n",
    "\n",
    "1. Load in necessary data processing packages\n",
    "2. Define the default parameters for data-prep object instantiation\n",
    "3. Load in the files containing the data processing parameters (datatypes,normalization, pca) \n",
    "4. Read, process, score the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "0bd157a2-3938-46ad-8e6a-ca68f984f170"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /db2joblib/full_pipeline_routine.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /db2joblib/full_pipeline_routine.py\n",
    "\n",
    "#1. Defining and importing necessary packages \n",
    "import nzae\n",
    "import json\n",
    "from joblib import dump, load\n",
    "import sys\n",
    "import pandas as pd\n",
    "# This is the transformation file\n",
    "sys.path.insert(0, '/mnt/backup/joblib/')\n",
    "import customer_segmentation_prep as transformer\n",
    "\n",
    "\n",
    "class full_pipeline(nzae.Ae):\n",
    "    def _runUdtf(self):\n",
    "        \n",
    "        #2.Defining parameters to instantiate \"data prep\" object \n",
    "        #-------------USER INPUT FIELDS-------------------#\n",
    "        # User input variables\n",
    "        effective_date = '2018-09-30'  # date at which the prediction was computed \n",
    "        train_or_score = 'score'\n",
    "        granularity_key='CUSTOMER_CUSTOMER_ID'\n",
    "        customer_start_date='CUSTOMER_SUMMARY_START_DATE'\n",
    "        customer_end_date='CUSTOMER_SUMMARY_END_DATE'\n",
    "        status_attribute='CUSTOMER_STATUS'\n",
    "        status_flag_active='Active'\n",
    "        date_customer_joined='CUSTOMER_RELATIONSHIP_START_DATE'\n",
    "\n",
    "        columns_required=['CUSTOMER_CUSTOMER_ID', 'CUSTOMER_STATUS', 'CUSTOMER_SUMMARY_START_DATE', 'CUSTOMER_SUMMARY_END_DATE',\n",
    "                            'CUSTOMER_EFFECTIVE_DATE',  'CUSTOMER_SYSTEM_LOAD_TIMESTAMP']\n",
    "\n",
    "        default_attributes=['CUSTOMER_GENDER', 'CUSTOMER_AGE_RANGE', 'CUSTOMER_EDUCATION_LEVEL','CUSTOMER_EMPLOYMENT_STATUS', 'CUSTOMER_MARITAL_STATUS', \n",
    "                            'CUSTOMER_URBAN_CODE', 'CUSTOMER_ANNUAL_INCOME', 'CUSTOMER_RELATIONSHIP_START_DATE', 'CUSTOMER_SUMMARY_RETURN_LAST_QUARTER', \n",
    "                            'CUSTOMER_SUMMARY_NUMBER_OF_EMAILS','CUSTOMER_SUMMARY_NUMBER_OF_LOGINS','CUSTOMER_SUMMARY_AMOUNT_OF_MANAGEMENT_FEES',\n",
    "                            'CUSTOMER_SUMMARY_TOP_SPENDING_CATEGORY', 'CUSTOMER_CREDIT_AUTHORITY_LEVEL', 'CUSTOMER_CUSTOMER_BEHAVIOR', \n",
    "                            'CUSTOMER_IMPORTANCE_LEVEL_CODE','CUSTOMER_MARKET_GROUP','CUSTOMER_PURSUIT']\n",
    "        \n",
    "        df_columns = columns_required + default_attributes\n",
    "        \n",
    "        risk_tolerance_list = []\n",
    "        investment_objective_list = []\n",
    "        std_multiplier=5\n",
    "        max_num_cat_cardinality=15\n",
    "        nulls_threshold=0.1\n",
    "        #-------------USER INPUT FIELDS-------------------#\n",
    "        \n",
    "        #3. Loading pca, object type, and normalization parameters\n",
    "        original_csv_dtypes = load('/mnt/backup/joblib/original_csv_dtypes.pkl')   #loading pkl file with datatypes \n",
    "        metadata_json = json.load( open('/mnt/backup/joblib/training_data_metadata.json','r')) #loading json file training metadata \n",
    "        pca = load('/mnt/backup/joblib/pca.txt') #loading pca file training metadata \n",
    "        \n",
    "        cols_to_standardise = metadata_json['cols_to_standardise']\n",
    "        scaler_means = metadata_json['scaler_means']\n",
    "        scaler_standard_dev = metadata_json['scaler_standard_dev']\n",
    "        cols_used_for_training = metadata_json['cols_used_for_training']\n",
    "        \n",
    "        self.model = load('/mnt/backup/joblib/kmeans_mdl.joblib')\n",
    "        \n",
    "        scoring_prep = transformer.CustomerSegmentationPrep('score', granularity_key=granularity_key,\n",
    "                                    customer_start_date=customer_start_date,\n",
    "                                    customer_end_date=customer_end_date,\n",
    "                                    status_attribute=status_attribute,\n",
    "                                    status_flag_active=status_flag_active,\n",
    "                                    date_customer_joined=date_customer_joined,\n",
    "                                    columns_required=columns_required,\n",
    "                                    default_attributes=default_attributes,\n",
    "                                    risk_tolerance_list=risk_tolerance_list,\n",
    "                                    investment_objective_list=investment_objective_list,\n",
    "                                    effective_date=effective_date,   \n",
    "                                    std_multiplier=std_multiplier,\n",
    "                                    max_num_cat_cardinality=max_num_cat_cardinality,\n",
    "                                    nulls_threshold=nulls_threshold)\n",
    "        \n",
    "        #4. Read in, process, and score the DB2 data in a loop (batch scoring)\n",
    "        batchsize = 10000\n",
    "        rownum = 0\n",
    "        row_list = []\n",
    "            \n",
    "        for row in self: \n",
    "\n",
    "            # Collect rows into batches\n",
    "            if self.isDone():\n",
    "                if rownum==0:\n",
    "                    break\n",
    "                # handle (last) paritally filled batch\n",
    "                batchsize = rownum\n",
    "            else:\n",
    "                row_list.append(row)\n",
    "                rownum = rownum+1         \n",
    "            \n",
    "            # Convert batches into DataFrames\n",
    "            if rownum==batchsize:\n",
    "                input_df = pd.DataFrame(row_list, columns=df_columns)\n",
    "            \n",
    "\n",
    "                #Defining the input fields back to their original python datatypes\n",
    "                input_df = input_df.astype(original_csv_dtypes)\n",
    "\n",
    "\n",
    "                #Execute transformation script\n",
    "                prepped_data = scoring_prep.prep_data(input_df, 'score')\n",
    "                ids = list(prepped_data['CUSTOMER_CUSTOMER_ID'])\n",
    "\n",
    "                if (prepped_data is not None):\n",
    "            \n",
    "                    # Perform feature scaling \n",
    "                    for i in range(0, len(cols_to_standardise)):\n",
    "                        current_col = cols_to_standardise[i]\n",
    "                        prepped_data[current_col] = (prepped_data[current_col] - scaler_means[i]) / scaler_standard_dev[i]   \n",
    "\n",
    "                    # if a column does not exist in scoring but is in training, add the column to scoring dataset\n",
    "                    for col in cols_used_for_training:\n",
    "                        if col not in list(prepped_data.columns):\n",
    "                            prepped_data[col] = 0\n",
    "                            \n",
    "                    # if a column exists in scoring but not in training, delete it from scoring dataset\n",
    "                    for col in list(prepped_data.columns):\n",
    "                        if col not in cols_used_for_training:\n",
    "                            prepped_data.drop(col, axis=1, inplace=True)\n",
    "\n",
    "                    prepped_data = prepped_data[cols_used_for_training]\n",
    "\n",
    "                    prepped_data = pd.DataFrame(pca.transform(prepped_data), columns=['pc_1','pc_2','pc_3','pc_4','pc_5','pc_6','pc_7','pc_8',\n",
    "                                                                                      'pc_9','pc_10','pc_11','pc_12','pc_13','pc_14'])\n",
    "                    \n",
    "                    prediction = self.model.predict(prepped_data) \n",
    "                    \n",
    "                    for x in range(len(prediction)):\n",
    "                        self.output(float(ids[x]),float(prediction[x]))\n",
    "                    \n",
    "                # prepare for next batch\n",
    "                row_list = []\n",
    "                rownum = 0\n",
    "                \n",
    "        self.done()\n",
    "\n",
    "        \n",
    "full_pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fdb83a3e-57a3-410c-8cae-f617c5bd1c92"
   },
   "source": [
    "Enable read/write access only if runnning for **first time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "03e4e07f-d098-41ae-8d86-bb3ba15fcf0e"
   },
   "outputs": [],
   "source": [
    "# # Providing permissions to execute UDF - only run this cell if running notebook for first time\n",
    "# !chmod -R 777 /db2joblib/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ae5c1742-fe63-4539-af1c-4b48c451b8d2"
   },
   "source": [
    "## 4. UDF Function Deployment\n",
    "\n",
    "### Writing and concatenating deployment strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "db2c05a4-ccfb-48b4-90c9-3bae9fe1538b"
   },
   "outputs": [],
   "source": [
    "# Define input column names\n",
    "columns_required=['CUSTOMER_CUSTOMER_ID', 'CUSTOMER_STATUS', 'CUSTOMER_SUMMARY_START_DATE', 'CUSTOMER_SUMMARY_END_DATE','CUSTOMER_EFFECTIVE_DATE',  \n",
    "                  'CUSTOMER_SYSTEM_LOAD_TIMESTAMP']\n",
    "\n",
    "default_attributes=['CUSTOMER_GENDER', 'CUSTOMER_AGE_RANGE', 'CUSTOMER_EDUCATION_LEVEL','CUSTOMER_EMPLOYMENT_STATUS', 'CUSTOMER_MARITAL_STATUS', \n",
    "                    'CUSTOMER_URBAN_CODE', 'CUSTOMER_ANNUAL_INCOME', 'CUSTOMER_RELATIONSHIP_START_DATE', 'CUSTOMER_SUMMARY_RETURN_LAST_QUARTER', \n",
    "                    'CUSTOMER_SUMMARY_NUMBER_OF_EMAILS','CUSTOMER_SUMMARY_NUMBER_OF_LOGINS','CUSTOMER_SUMMARY_AMOUNT_OF_MANAGEMENT_FEES',\n",
    "                    'CUSTOMER_SUMMARY_TOP_SPENDING_CATEGORY', 'CUSTOMER_CREDIT_AUTHORITY_LEVEL', 'CUSTOMER_CUSTOMER_BEHAVIOR', \n",
    "                    'CUSTOMER_IMPORTANCE_LEVEL_CODE','CUSTOMER_MARKET_GROUP','CUSTOMER_PURSUIT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "4721b384-2ca8-4097-8413-1ecaf20d079e"
   },
   "outputs": [],
   "source": [
    "# Query Db2 for input column datatypes - used for creating the UDF input datatypes string\n",
    "sql = \"\"\"SELECT NAME, COLTYPE,LENGTH FROM SYSIBM.SYSCOLUMNS WHERE TBCREATOR='DSE' AND TBNAME='CUST_SEG_DATA_TEST'\"\"\"\n",
    "dtypes_df = pd.read_sql(sql, dbi_bluedb_connection)\n",
    "\n",
    "# filter based on input columns we use (24 out of ~300 available)\n",
    "dtypes_df=dtypes_df[dtypes_df['NAME'].isin(columns_required+default_attributes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "dcc13e29-1abf-4382-855c-badd9252cd50"
   },
   "outputs": [],
   "source": [
    "# Create Db2 datatype mapping for UDF input columns\n",
    "mapping = [str(dtypes_df['COLTYPE'][dtypes_df['NAME']==x].values[0]).strip()+'('+str(dtypes_df['LENGTH'][dtypes_df['NAME']==x].values[0])+')' \n",
    "           if str(dtypes_df['COLTYPE'][dtypes_df['NAME']==x].values[0]).strip()==\"VARCHAR\" \n",
    "           else str(dtypes_df['COLTYPE'][dtypes_df['NAME']==x].values[0]).strip() \n",
    "           for x in (columns_required + default_attributes)]\n",
    "\n",
    "# Create a string from the mapping. This is used in the deployment statement\n",
    "input_dtypes_string = ', '.join([x for x in mapping ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "db1a84c6-32fe-44c8-8b3d-6ae624ebef93"
   },
   "source": [
    "### Function Deployment \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "15f2c066-c6c4-4c61-9924-dbe8b51dd3c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CREATE OR REPLACE FUNCTION full_pipeline(DOUBLE, VARCHAR(90), VARCHAR(90), VARCHAR(90), VARCHAR(90), DOUBLE, VARCHAR(90), VARCHAR(90), VARCHAR(90), VARCHAR(90), VARCHAR(90), VARCHAR(90), DOUBLE, VARCHAR(90), DOUBLE, DOUBLE, DOUBLE, VARCHAR(90), VARCHAR(90), VARCHAR(90), VARCHAR(90), VARCHAR(90), VARCHAR(90), VARCHAR(90)) returns table (ID double, PREDICTION double) language PYTHON  parameter style NPSGENERIC  FENCED  NOT THREADSAFE  NO FINAL CALL  DISALLOW PARALLEL  NO DBINFO  DETERMINISTIC NO EXTERNAL ACTION RETURNS NULL ON NULL INPUT  NO SQL external name '/mnt/backup/joblib/full_pipeline_routine.py'\n"
     ]
    }
   ],
   "source": [
    "# Deploy UDF function\n",
    "create_udf_function_full_pipeline = \"\"\"\n",
    "CREATE OR REPLACE FUNCTION full_pipeline({0}) returns table (ID double, PREDICTION double) language PYTHON  parameter style \\\n",
    "NPSGENERIC  FENCED  NOT THREADSAFE  NO FINAL CALL  DISALLOW PARALLEL  NO DBINFO  DETERMINISTIC \\\n",
    "NO EXTERNAL ACTION RETURNS NULL ON NULL INPUT  NO SQL external name '/mnt/backup/joblib/full_pipeline_routine.py'\"\"\".format(input_dtypes_string)\n",
    "ibm_db.exec_immediate(bluedb_connection,create_udf_function_full_pipeline)\n",
    "\n",
    "print(create_udf_function_full_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0edf4055-6da2-437f-9aa9-0269b293cc22"
   },
   "source": [
    "## 5. Make Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "f6702d6b-ad81-4451-bf4d-1125a72888e5"
   },
   "outputs": [],
   "source": [
    "## Writing column selection string for the input. This is used in the SQL selection queries. \n",
    "columns_selection_sql= ', '.join(str(x) for x in columns_required + default_attributes)\n",
    "# Add \"i.\" to input columns for use in the SQL statement\n",
    "input_columns = ', '.join('i.{}'.format(x) for x in columns_selection_sql.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "cceae11b-9ffe-41ad-986c-50052e906550"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop existing results table\n",
    "sql_drop = 'DROP TABLE DSE.CUST_SEG_PREDICTIONS'\n",
    "stmt_drop = ibm_db.prepare(bluedb_connection, sql_drop)\n",
    "\n",
    "ibm_db.execute(stmt_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "55ddfd63-8308-4dd7-896f-7095d302c2d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create table for storing predictions\n",
    "sql = 'CREATE TABLE DSE.CUST_SEG_PREDICTIONS(CUST_ID DOUBLE, PREDICTION DOUBLE);'\n",
    "\n",
    "stmt = ibm_db.prepare(bluedb_connection, sql)\n",
    "\n",
    "ibm_db.execute(stmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "1f912866-9bdb-4235-a0ad-e4e7b8784c43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSERT INTO DSE.CUST_SEG_PREDICTIONS(CUST_ID, PREDICTION) select f.* from DSE.CUST_SEG_DATA_TEST i, table(full_pipeline(i.CUSTOMER_CUSTOMER_ID, i. CUSTOMER_STATUS, i. CUSTOMER_SUMMARY_START_DATE, i. CUSTOMER_SUMMARY_END_DATE, i. CUSTOMER_EFFECTIVE_DATE, i. CUSTOMER_SYSTEM_LOAD_TIMESTAMP, i. CUSTOMER_GENDER, i. CUSTOMER_AGE_RANGE, i. CUSTOMER_EDUCATION_LEVEL, i. CUSTOMER_EMPLOYMENT_STATUS, i. CUSTOMER_MARITAL_STATUS, i. CUSTOMER_URBAN_CODE, i. CUSTOMER_ANNUAL_INCOME, i. CUSTOMER_RELATIONSHIP_START_DATE, i. CUSTOMER_SUMMARY_RETURN_LAST_QUARTER, i. CUSTOMER_SUMMARY_NUMBER_OF_EMAILS, i. CUSTOMER_SUMMARY_NUMBER_OF_LOGINS, i. CUSTOMER_SUMMARY_AMOUNT_OF_MANAGEMENT_FEES, i. CUSTOMER_SUMMARY_TOP_SPENDING_CATEGORY, i. CUSTOMER_CREDIT_AUTHORITY_LEVEL, i. CUSTOMER_CUSTOMER_BEHAVIOR, i. CUSTOMER_IMPORTANCE_LEVEL_CODE, i. CUSTOMER_MARKET_GROUP, i. CUSTOMER_PURSUIT)) f\n"
     ]
    }
   ],
   "source": [
    "execute_udf_sql = 'INSERT INTO DSE.CUST_SEG_PREDICTIONS(CUST_ID, PREDICTION) select f.* from DSE.CUST_SEG_DATA_TEST i, table(full_pipeline({})) f'.format(input_columns)\n",
    "print(execute_udf_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "9579de16-017f-45aa-a297-60c4462bb0a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stmt = ibm_db.prepare(bluedb_connection, execute_udf_sql)\n",
    "ibm_db.execute(stmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "f3d478a3-fa7a-434e-83a5-9a87bb686316"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CUST_ID</th>\n",
       "      <th>PREDICTION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CUST_ID  PREDICTION\n",
       "0      0.0         4.0\n",
       "1      1.0         4.0\n",
       "2      2.0         6.0\n",
       "3      3.0         4.0\n",
       "4      4.0         6.0\n",
       "5      5.0         5.0\n",
       "6      6.0         4.0\n",
       "7      7.0         6.0\n",
       "8      8.0         4.0\n",
       "9      9.0         5.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql = 'SELECT * FROM DSE.CUST_SEG_PREDICTIONS ORDER BY CUST_ID'\n",
    "df = pd.read_sql(sql, dbi_bluedb_connection)\n",
    "df.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
